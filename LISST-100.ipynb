{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c71cbc",
   "metadata": {},
   "source": [
    "# LISST-100 data processing notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e947d15",
   "metadata": {},
   "source": [
    "The following notebook was developed by the Suspended Material and Seabed Monitoring and Modelling group (SUMO) from the Institute of Natural Sciences, Belgium. It aims at providing a standard protocol to process data collected in-situ with a LISST-100 instrument placed on a rosette profiling the water column. To use this notebook, simply import the data previously converted to asc by the LISST-SOPv5 software, run the cells containing code and inspect the results obtained. The graphs can then be saved as jpg and the processed data as csv.\n",
    "\n",
    "In this notebook, the LISST-100 data processing has been divided into three stages:\n",
    "1. *ASC data generation*: The first stage, during which the raw data is converted from .dat to .asc, requires the use of the LISST-SOPv5 software. \n",
    "2. *Data import & flagging*: This notebook can then be used to carry out the second stage, which includes the import and pre-processing of the data (i.e. date conversion, data flagging). At the end of this stage, the user can save the full dataset with a flagging system and dates as datetime. \n",
    "3. *Statistics computation & visualization*: The third stage performs the calculation of different statistical parameters on a clean dataset (after filtrating data above a certain flag value), such as mean particle diameter, D10, D50, D90 and distribution characteristics, and displays outputs as graphs. The clean dataset as well as the graphs can be saved. Data are also merged per cast, allowing the user to save a csv containing mean values of each cast.\n",
    "\n",
    "**Important!** Before starting the processing, make sure that all the necessary packages and libraries are installed on your computer and that you run the cell below to import everything that is required.\n",
    "**Important!!** The beam attenuation is computed assuming the path length is 50 mm and is not automatically adapted when using a path reduction module! This option will be added in a further version of this code. As indicated in the LISST-100 user manual, these are the expressions used to convert:\n",
    "- c = -ln(optical transmission)x(1/0.025 m) if you are using a 50% PRM (25 mm path length)\n",
    "- c = -ln(optical transmission)x(1/0.01 m) if you are using an 80% PRM (10 mm path length) \n",
    "- c = -ln(optical transmission)x(1/0.005 m) if you are using a 90% PRM (5 mm path length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 01 ##\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from tkinter.filedialog import asksaveasfilename\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c57fbf",
   "metadata": {},
   "source": [
    "## Stage 1: Conversion of dat files to asc files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16150704",
   "metadata": {},
   "source": [
    "In this stage, the conversion of the raw LISST-100 data is performed using the LISST-SOPv5 software, which can be downloaded freely on https://www.sequoiasci.com/product/lisst-100x/. The different steps to be perfomed are explained in more details in this section. \n",
    "\n",
    "1. Open .dat file with corresponding background file (.asc) and select output directory. Once that is done, a graph will appear showing the different rings (from 1 to 32) through time, as well as another graph showing the auxiliary parameters (if any) through time. In your output directory, you will notice that new files have been added: a .psd and a .log files (for both round particles and random shapes (rs)).\n",
    "\n",
    "2. After inspection of the data, you can then click on the \"Process File\" button. Be aware that depending on the size of your data, that operation might take a long time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d821fc3",
   "metadata": {},
   "source": [
    "## Stage 2: Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fbf22",
   "metadata": {},
   "source": [
    "#### Stage 2 - Step 1 : Enter metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81d228",
   "metadata": {},
   "source": [
    "In this cell, the user will be asked to enter the campaign identification code, year at which the data were collected, the name of the sampling location and coordinates in decimal degrees (XX.XXXXX). Press enter after each input.\n",
    "**Important!** Avoid using \"/\" in the campaign identification code, as it might create issues when saving the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 02 ##\n",
    "campaign_code = input(\"Enter the campaign identification code: \")\n",
    "year = int(input(\"Enter the year of the data: \"))\n",
    "location = input(\"Enter the name of the sampling location: \")\n",
    "latitude = input(\"Enter the latitude of the data: \")\n",
    "longitude = input(\"Enter the longitude of the data: \")\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf1367",
   "metadata": {},
   "source": [
    "#### Stage 2 - Step 2 : Data import & formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46108881",
   "metadata": {},
   "source": [
    "Upon running this cell, a new window should automatically open and invite the user to select the asc file to be processed. Be aware that this window might open in the background and could be hidden under the current window. Once imported, the data will be attributed column names, campaign identification number, sampling location and year (as inserted by the user in the cell above). In addition to that, day_hour and minute_second columns will be converted to jday and datetime values. A new column containing a flag value (set by default to zero at this step) will be added at the end of the dataset.\n",
    "\n",
    "The dataset will be displayed upon running the cell, enabling the user to check that columns and values were assigned correctly and that the calculated date and time of the data correspond to the actual sampling time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 03 ##\n",
    "# Open a window to select input file\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "file_path = filedialog.askopenfilename(title=\"Select an ASC file\", filetypes=[(\"ASC files\", \"*.asc\")])\n",
    "directory_path = os.path.dirname(file_path)\n",
    "\n",
    "output_directory = f\"{directory_path}/LISST-100-{campaign_code}-{location}-processed\"\n",
    "if os.path.exists(output_directory):\n",
    "    shutil.rmtree(output_directory)\n",
    "    print(f'Existing output folder removed at: {output_directory}')\n",
    "os.makedirs(output_directory)\n",
    "print(f'Output folder created at: {output_directory}')\n",
    "\n",
    "if file_path:  \n",
    "    try:\n",
    "        data = pd.read_csv(file_path, header=None, sep=\" \")\n",
    "        print(\"File successfully loaded!\") \n",
    "        data = data.iloc[:, 0:-1]\n",
    "        # Add column names\n",
    "        data.columns = [\"2.72\",\"3.2\",\"3.78\",\"4.46\",\"5.27\",\"6.21\",\"7.33\",\"8.65\",\"10.2\",\"12.1\",\"14.2\",\"16.8\",\"19.8\",\"23.4\",\"27.6\",\"32.5\",\"38.4\",\"45.3\",\"53.5\",\"63.1\",\"74.5\",\"87.9\",\"104\",\"122\",\"144\",\"170\",\"201\",\"237\",\"280\",\"331\",\"390\",\"460\",\"laser_transmission_sensor_mW\",\"battery_voltage_V\",\"external_input_1_mW\",\"laser_reference_sensor_mW\",\"pressure_m\",\"temperature_C\",\"day_hour\",\"minute_second\",\"optical_transmission\",\"beam_attenuation_m\"]\n",
    "        # Add campaign identification number, sampling location name, latitude, longitude and year\n",
    "        data['campaign'] = campaign_code\n",
    "        data['location'] = location\n",
    "        data['latitude'] = latitude\n",
    "        data['longitude'] = longitude\n",
    "        data['year'] = year\n",
    "        # Calculate datetime\n",
    "        data['jday'] = round((data['day_hour'] / 100),0)\n",
    "        start_of_year = datetime(year, 1, 1)\n",
    "        data['datetime'] = pd.to_datetime(data['jday'].apply(lambda x: start_of_year + timedelta(days=x-1))) \\\n",
    "    + pd.to_timedelta((data['day_hour'] % 100), unit='h') \\\n",
    "    + pd.to_timedelta(data['minute_second'] // 100, unit='m') \\\n",
    "    + pd.to_timedelta(data['minute_second'] % 100, unit='s')\n",
    "        data['flag'] = 0\n",
    "        # Compute sampling start and stop datetime\n",
    "        sampling_start = data['datetime'].min()\n",
    "        sampling_stop = data['datetime'].max()\n",
    "        # Display output\n",
    "        print(f'Data were collected at {location} between {sampling_start} and {sampling_stop} during campaign {campaign_code}.')\n",
    "        display(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "else:\n",
    "    print(\"No file selected\")\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a03de",
   "metadata": {},
   "source": [
    "#### Stage 2 - Step 3: Flagging of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008bcbc",
   "metadata": {},
   "source": [
    "In this cell, flagging operations are performed as follows:\n",
    "1. A first check is done on the **reference laser**, if values are below 0.2 mW, this indicates that the laser is likely not working properly. Quality flags will be assigned to 1 if data are above this value and to 3 if not (see flags meaning below). If more than 10% of the data present such a low value, a warning message will be displayed.  \n",
    "2. Data out of the water are flagged with the value of 4 (rows where the beam attenuation is equal to zero). The **beam attenuation** corresponds to the loss of light intensity when a laser beam passes through water, due to both absorption and scattering. Higher attenuation values typically indicate more turbid water. \n",
    "3. The **laser transmission** values are checked. This indicates how much of the laser light has passed through water without being absorbed or scattered. A value of 100% indicates that there has been no light loss through the sample (either water without particles or air) while values below indicate scattering and absorption (which is expected in seawater). Values above 100%, however, indicate a sensor malfunction or miscalibration. In this cell, a first inspection of the data calculates how many data are above 100% and if that number is acceptable, these rows are removed from the data. If most data (>50%) are above 100%, then a warning message is generated. Data above 100 are flagged with the value of 3. \n",
    "4. The **optical transmission** is checked: values should be ranging between 0 and 1. Values outside that range should be discarded (flag 4). However, values above 0.98-0.995 reflect extremely clear water conditions, meaning a low signal-to-noise ratio (flag 3, to be taken with caution. Above that range, values should be discarded), whereas values below 0.10 reflect very turbid data and should be discarded. \n",
    "5. Finally, a flag of 3 is attributed to **outliers** detected based on the total volume concentration calculated.\n",
    "\n",
    "Once the flagging has been performed, a graph allows the user to visualize the quality of the data. The complete flagged dataset is automatically saved in the output directory selected by the user at the beginning of this notebook. In stage 3, a cell then allows to filter out all the data with a quality flag equal or higher than 4.\n",
    "\n",
    "Quality flags are defined following the quality flags standards defined by the NERC Environmental Data Service of the British Oceanographic Data Centre (https://vocab.nerc.ac.uk/collection/L20/current/):\n",
    "0: No quality control\n",
    "1: Good value\n",
    "2: Probably good value\n",
    "3: Probably bad value\n",
    "4: Bad value\n",
    "5: Changed value\n",
    "6: Value below detection\n",
    "7: Value in excess\n",
    "8: Interpolated value\n",
    "9: Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 04 ##\n",
    "# Step 1: Flagging based on reference_laser\n",
    "data['flag'] = data.apply(lambda row: max(row['flag'], 1) if row['laser_reference_sensor_mW'] > 0.02 else max(row['flag'], 3), axis=1)\n",
    "reference_below_thsld = (data['laser_reference_sensor_mW'] <= 0.02).sum() / len(data) * 100\n",
    "if reference_below_thsld > 20:  # If more than 20% of the data are below 0.02 mW\n",
    "    messagebox.showwarning(\"Laser should be checked\", \n",
    "                           f\"Warning: {reference_below_thsld:.0f}% of the data have a laser reference value below 0.02 mW. \"\n",
    "                           \"Please check laser.\")\n",
    "\n",
    "# Step 2: Beam attenuation flagging\n",
    "data['flag'] = data.apply(lambda row: max(row['flag'], 4) if row['beam_attenuation_m'] <= 0 else row['flag'], axis=1)\n",
    "\n",
    "# Step 3: Laser transmission flagging\n",
    "data['flag'] = data.apply(lambda row: 3 if row['laser_transmission_sensor_mW'] > 100 and row['flag'] < 3 else row['flag'], axis=1)\n",
    "underwater_data = data[data['beam_attenuation_m'] > 0]\n",
    "above_100_percent = (underwater_data['laser_transmission_sensor_mW'] > 100).sum() / len(underwater_data) * 100\n",
    "above_100_percent = (data['laser_transmission_sensor_mW'] > 100).sum() / len(data) * 100\n",
    "if above_100_percent > 50:  # If more than 50% of the data are above 100%\n",
    "    messagebox.showwarning(\"Recalibration Needed\", \n",
    "                           f\"Warning: {above_100_percent:.0f}% of the underwater data are above 100% transmission. \"\n",
    "                           \"Please recalibrate the sensor.\")\n",
    "    \n",
    "# Step 4: Optical transmission flagging\n",
    "data['flag'] = data.apply(lambda row: 3 if row['optical_transmission'] > 0.98 and row['optical_transmission'] < 0.995 and row['flag'] < 3 else row['flag'], axis=1)\n",
    "data['flag'] = data.apply(lambda row: 4 if row['optical_transmission'] >= 0.995 and row['flag'] < 4 else row['flag'], axis=1)\n",
    "data['flag'] = data.apply(lambda row: 4 if row['optical_transmission'] <= 0.10 and row['flag'] < 4 else row['flag'], axis=1)\n",
    "    \n",
    "# Step 5: Outlier detection using IQR\n",
    "volume_columns = ['2.72', '3.2', '3.78', '4.46', '5.27', '6.21', '7.33', '8.65', \n",
    "                  '10.2', '12.1', '14.2', '16.8', '19.8', '23.4', '27.6', '32.5', \n",
    "                  '38.4', '45.3', '53.5', '63.1', '74.5', '87.9', '104', '122', \n",
    "                  '144', '170', '201', '237', '280', '331', '390', '460']\n",
    "\n",
    "data['total_volume_concentration'] = data[volume_columns].sum(axis=1)\n",
    "Q1 = data['total_volume_concentration'].quantile(0.25)\n",
    "Q3 = data['total_volume_concentration'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "data['flag'] = data.apply(lambda row: 3 if row['total_volume_concentration'] < lower_bound or row['total_volume_concentration'] > upper_bound else row['flag'], axis=1)\n",
    "data['is_outlier'] = (data['total_volume_concentration'] < lower_bound) | (data['total_volume_concentration'] > upper_bound)\n",
    "outlier_count = data['is_outlier'].sum()\n",
    "print(f'Number of outliers detected: {outlier_count}')\n",
    "    \n",
    "# Warning message quality flags\n",
    "percentage_high_flags = (data['flag'] >= 3).sum() / len(data) * 100\n",
    "if percentage_high_flags > 75:  # If more than 75% of the data have a quality flag of 3 or higher\n",
    "    messagebox.showwarning(\"Unsatisfactory data quality\", \n",
    "                           f\"Warning: {percentage_high_flags:.0f}% of the data have a quality flag above 3. \"\n",
    "                           \"Data should be used with caution.\")\n",
    "    \n",
    "# Output\n",
    "print('Flagging has been successfully performed on the complete dataset:')\n",
    "display(data)\n",
    "\n",
    "flag_counts = data['flag'].value_counts().sort_index()\n",
    "colors = {1: 'green', 2: 'yellow', 3:'orange', 4: 'red'}\n",
    "fig, ax = plt.subplots()\n",
    "flag_counts.plot(kind='bar', color=[colors.get(flag, 'blue') for flag in flag_counts.index], ax=ax)\n",
    "ax.grid(axis='both', which='both', linewidth=0.3)\n",
    "ax.set_title(\"Count per flag category\")\n",
    "ax.set_xlabel(\"Flag\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizing outliers\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(data.index, data['total_volume_concentration'], label='', color='blue', s=10, alpha=0.5)\n",
    "plt.scatter(data[data['is_outlier']].index, data[data['is_outlier']]['total_volume_concentration'], \n",
    "            label='Outliers', color='red', marker='x', s=15)\n",
    "plt.axhline(lower_bound, color='orange', linestyle='--', label='Lower limit')\n",
    "plt.axhline(upper_bound, color='orange', linestyle='--', label='Upper limit')\n",
    "plt.grid(axis='both', which='both', linewidth=0.3)\n",
    "plt.title('Outlier detection')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Total volume concentration (µl/l)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "data.drop(columns=['total_volume_concentration'], inplace=True)\n",
    "\n",
    "# Save as csv\n",
    "data_path = f\"{output_directory}/{campaign_code}-{location}-LISST100-full-data.csv\"\n",
    "if data_path:\n",
    "    data.to_csv(data_path, index=False)\n",
    "    print(f\"File saved to {file_path}\")\n",
    "else:\n",
    "    print(\"Save operation cancelled.\")\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6156e4",
   "metadata": {},
   "source": [
    "## Stage 3: Statistics computation & visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e8a0d",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 1: Statistics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653d32c",
   "metadata": {},
   "source": [
    "By running this cell, a series of statistics are calculated on the full dataset. This includes:\n",
    "1. **Total volume concentration**: This sums up the volume concentration of each grain size class per row.\n",
    "2. **Relative volume concentration**: This calculates the percentage that each class represents compared to the total volume concentration.\n",
    "3. **Mean diameter**: This corresponds to the mean diameter of particles in each row weighted by their volume concentration and normalized by the total volume concentration.\n",
    "4. **D10, D50, D90 values**: These represent the diameter below which 10%, 50% or 90% of the volume of the data are found. The D10 helps characterizing the finer fraction of the sample, the D50 corresponds to the median diameter and the D90 characterizes the coarser sediment fraction. These values are calculated based on the cumulative distribution.\n",
    "5. **Span**: It is a measure of the sorting of particle sizes, as a normalized measure of the distribution spread around the median particle size, showing the relative range of the middle 80% of the particle size distribution. Small values indicate good sorting (close to 1) while a larger span indicates poor sorting of the particles.\n",
    "6. **Standard deviation**: It measures the average dispersion of particle sizes from the mean, a greater standard deviation shows greater variability around the mean with possible outliers or tails. If the span and standard deviation are low, it shows a uniform distribution; if the span is moderate but the standard deviation is high, it shows a moderate spread with outliers; and if the span and standard deviation are high, it shows a wide distribution.\n",
    "7. **Mode**: The mode is the particle size that has the largest volume of the distribution (peak in the distribution).\n",
    "8. **Peaks in the distribution**: It identifies peaks in the particle size distribution.\n",
    "\n",
    "The particle size distribution combined with the cumulative volume concentration is displayed upon running of this cell as well as a table with the mean value of all the calculated parameters. The graph and updated complete clean dataframe are automatically saved in the directory selected by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82582c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 07 ##  \n",
    "# Calculate the total volume concentration \n",
    "volume_concentration_columns = ['2.72', '3.2', '3.78', '4.46', '5.27', '6.21', '7.33', '8.65', \n",
    "                  '10.2', '12.1', '14.2', '16.8', '19.8', '23.4', '27.6', '32.5', \n",
    "                  '38.4', '45.3', '53.5', '63.1', '74.5', '87.9', '104', '122', \n",
    "                  '144', '170', '201', '237', '280', '331', '390', '460']\n",
    "average_volume_fractions = data[volume_concentration_columns].mean()\n",
    "data['total_volume_concentration_ul_l'] = data[volume_concentration_columns].sum(axis=1)\n",
    "\n",
    "# Calculate the percentage of each class compared to the total volume concentration\n",
    "for col in volume_concentration_columns:\n",
    "    data[f'{col}_%'] = ((data[col] / data['total_volume_concentration_ul_l']) * 100).astype(float)\n",
    "\n",
    "# Calculate mean diameter \n",
    "volume_concentrations = data[volume_concentration_columns].values\n",
    "grain_sizes = np.array([float(col) for col in volume_concentration_columns])\n",
    "data['mean_diameter_um'] = np.sum(volume_concentrations * grain_sizes, axis=1) / data['total_volume_concentration_ul_l']\n",
    "\n",
    "# Cumulative volume concentration\n",
    "cumulative_volumes = np.cumsum(data[volume_concentration_columns].values, axis=1)\n",
    "\n",
    "# Percentiles calculation function\n",
    "def calculate_d_percentile(cumulative_volumes, grain_sizes, total_volume, percentile):\n",
    "    target = total_volume * (percentile / 100.0)\n",
    "    d_percentile = []\n",
    "    for i in range(len(total_volume)):\n",
    "        if total_volume.iloc[i] == 0: \n",
    "            d_percentile.append(np.nan)\n",
    "        else:\n",
    "            greater_equal_idx = np.argmax(cumulative_volumes[i, :] >= target.iloc[i])\n",
    "            d_percentile.append(grain_sizes[greater_equal_idx])\n",
    "    return np.array(d_percentile)\n",
    "\n",
    "# Calculate D10, D50, D90\n",
    "data['D10_um'] = calculate_d_percentile(cumulative_volumes, grain_sizes, data['total_volume_concentration_ul_l'], 10)\n",
    "data['D50_um'] = calculate_d_percentile(cumulative_volumes, grain_sizes, data['total_volume_concentration_ul_l'], 50)\n",
    "data['D90_um'] = calculate_d_percentile(cumulative_volumes, grain_sizes, data['total_volume_concentration_ul_l'], 90)\n",
    "\n",
    "# Calculate the span\n",
    "data['span'] = (data['D90_um'] - data['D10_um']) / data['D50_um']\n",
    "\n",
    "# Calculate the standard deviation\n",
    "def calculate_weighted_std(grain_sizes, volume_concentrations, mean_diameter):\n",
    "    variance = np.sum(volume_concentrations * (grain_sizes - mean_diameter[:, None])**2, axis=1) / np.sum(volume_concentrations, axis=1)\n",
    "    return np.sqrt(variance)\n",
    "data['std_dev_um'] = calculate_weighted_std(grain_sizes, volume_concentrations, data['mean_diameter_um'].values)\n",
    "\n",
    "# Calculate the mode\n",
    "def calculate_mode(grain_sizes, volume_concentrations):\n",
    "    mode_values = []\n",
    "    for vc in volume_concentrations:\n",
    "        if np.all(vc == 0):\n",
    "            mode_values.append(np.nan)\n",
    "        else:\n",
    "            mode_index = np.argmax(vc)\n",
    "            mode_values.append(grain_sizes[mode_index])\n",
    "    return np.array(mode_values)\n",
    "data['mode_um'] = calculate_mode(grain_sizes, volume_concentrations)\n",
    "\n",
    "# Identify peaks in the distribution \n",
    "def find_all_peaks(grain_sizes, volume_concentrations):\n",
    "    all_peaks = []\n",
    "    for vc in volume_concentrations:\n",
    "        if np.all(vc == 0):\n",
    "            all_peaks.append([])\n",
    "        else:\n",
    "            peaks, _ = find_peaks(vc)\n",
    "            peak_sizes = grain_sizes[peaks]\n",
    "            all_peaks.append(peak_sizes.tolist())\n",
    "    return all_peaks\n",
    "data['peaks'] = find_all_peaks(grain_sizes, volume_concentrations)\n",
    "\n",
    "# Display updated dataframe\n",
    "print('Statistics have been successfully computed')\n",
    "display(data)\n",
    "\n",
    "# Display the histogram of the particle size distribution and the cumulative volume distribution (in red)\n",
    "total_volume_concentration_per_class = data[volume_concentration_columns].sum(axis=0)\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6), dpi=300)\n",
    "ax1.plot(grain_sizes, total_volume_concentration_per_class, color='blue', marker='o', \n",
    "         label='Particle size distribution', linestyle='-')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Grain size (µm)')\n",
    "ax1.set_ylabel('Total volume concentration (µl/l)')\n",
    "ax1.set_title(f'LISST-100 {location} {campaign_code}')\n",
    "ax1.grid(axis='both', which='both', linewidth=0.3)\n",
    "cumulative_volumes = np.cumsum(total_volume_concentration_per_class)\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(grain_sizes, cumulative_volumes / cumulative_volumes[-1] * 100, color='red', \n",
    "         label='Cumulative Distribution', marker='o')\n",
    "ax2.set_ylabel('Cumulative volume (%)', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax1.axvline(data['D10_um'].mean(), color='green', linestyle='--', label='D10')\n",
    "ax1.axvline(data['D50_um'].mean(), color='orange', linestyle='--', label='D50')\n",
    "ax1.axvline(data['D90_um'].mean(), color='purple', linestyle='--', label='D90')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display table with the mean of each parameter\n",
    "mean_values = {\n",
    "    'Total volume concentration (µl/l)': data['total_volume_concentration_ul_l'].mean(),\n",
    "    'Mean diameter (µm)': data['mean_diameter_um'].mean(),\n",
    "    'D10 (µm)': data['D10_um'].mean(),\n",
    "    'D50 (µm)': data['D50_um'].mean(),\n",
    "    'D90 (µm)': data['D90_um'].mean(),\n",
    "    'Span': data['span'].mean(),\n",
    "    'Standard deviation (µm)': data['std_dev_um'].mean(),\n",
    "    'Mode (µm)': data['mode_um'].mean()\n",
    "}\n",
    "mean_values_df = pd.DataFrame(mean_values, index=[0])\n",
    "print('Mean values of calculated parameters for the complete clean dataset:')\n",
    "display(mean_values_df)\n",
    "\n",
    "# Save output\n",
    "graph_file_path = f\"{output_directory}/{campaign_code}-{location}-LISST100-full-PSD.png\"\n",
    "fig.savefig(graph_file_path) \n",
    "print(f'Graph is saved to: {output_directory}')\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a2e9c",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 2: Removal of suspicious data & creation of a 'clean' dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc35311",
   "metadata": {},
   "source": [
    "This step allows the user to choose a certain flag threshold at and above which data are discarded for further analysis. The default value is set to 4, meaning all data flagged 4 and above will be removed from the filtered dataframe. In this cell, values considered outliers can also be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 06 ##\n",
    "try:\n",
    "    threshold = int(input(\"Enter the threshold for flag filtration (default is 4): \") or 4)\n",
    "except ValueError:\n",
    "    print(\"Invalid input, defaulting to threshold = 4.\")\n",
    "    threshold = 4\n",
    "    \n",
    "outliers = input(\"Remove outliers (yes or no): \")\n",
    "\n",
    "# Filter out rows where flag is greater than or equal to the chosen threshold\n",
    "filtered_data = data[data['flag'] < threshold]\n",
    "\n",
    "# Filter out ouliers\n",
    "if outliers == 'yes' or outliers == 'Yes':\n",
    "    filtered_data = filtered_data[filtered_data['is_outlier'] != True]\n",
    "elif outliers == 'no' or outliers == 'No':\n",
    "    filtered_data = filtered_data\n",
    "else:\n",
    "    print(\"Invalid input, outliers were not filtered out.\")\n",
    "\n",
    "# Display the filtered data\n",
    "print(f\"After filtering, the dataset is:\")\n",
    "display(filtered_data)\n",
    "\n",
    "# Save filtered data as csv\n",
    "filtered_data_path = f\"{output_directory}/{campaign_code}-{location}-LISST100-clean-data.csv\"\n",
    "filtered_data.to_csv(filtered_data_path, index=False)\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70540308",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 3: Division of the clean dataset into casts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148dd96",
   "metadata": {},
   "source": [
    "In this cell, a cast letter will be attributed to each set of consecutive values separated from the previous set by at least 20 minutes. If needed, this time difference can be adapted in the code directly (line 8). If the user has other data for these casts, we advise comparing their timings to the table generated in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 08 ##\n",
    "data = filtered_data if not filtered_data.empty else data\n",
    "\n",
    "current_cast = 'A'\n",
    "cast_numbers = [current_cast]\n",
    "\n",
    "# Loop through the dataset and check the time difference between consecutive rows\n",
    "for i in range(1, len(data)):\n",
    "    time_diff = data['datetime'].iloc[i] - data['datetime'].iloc[i - 1]\n",
    "    # If the time difference is greater than or equal to 20 minutes, increment the cast value\n",
    "    if time_diff >= timedelta(minutes=10):\n",
    "        current_cast = chr(ord(current_cast) + 1)\n",
    "    cast_numbers.append(current_cast)\n",
    "\n",
    "# Add the cast numbers as a new column in the dataframe\n",
    "data.loc[:, 'cast'] = cast_numbers\n",
    "\n",
    "# Group by the cast to find the start and stop time for each cast\n",
    "summary_table = data.groupby('cast').agg(\n",
    "    datetime_start=('datetime', 'min'),\n",
    "    datetime_stop=('datetime', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Display the summary table\n",
    "print('Summary of casts timings (clean data):')\n",
    "display(summary_table)\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d438c",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 4: Comparison of the different casts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e530b0",
   "metadata": {},
   "source": [
    "This cell generates a table and several graphs comparing the different casts of the processed dataset. These are saved in the directory previously selected by the user, together with a csv containing the mean values of each cast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc008b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 09 ##\n",
    "# Create summary table per cast with mean values\n",
    "summary_table = data.groupby('cast').agg(\n",
    "    datetime_start=('datetime', 'min'),\n",
    "    datetime_stop=('datetime', 'max'),\n",
    "    mean_total_volume_concentration=('total_volume_concentration_ul_l', 'mean'),\n",
    "    mean_diameter=('mean_diameter_um', 'mean'),\n",
    "    mean_D10=('D10_um', 'mean'),\n",
    "    mean_D50=('D50_um', 'mean'),\n",
    "    mean_D90=('D90_um', 'mean'),\n",
    "    mean_span=('span', 'mean'),\n",
    "    mean_std_dev=('std_dev_um', 'mean'),\n",
    "    mean_mode=('mode_um', 'mean'),\n",
    ").reset_index()\n",
    "summary_table_path = f\"{output_directory}/{campaign_code}-{location}-LISST100-cast-summary.csv\"\n",
    "summary_table.to_csv(summary_table_path, index=False)\n",
    "print(f\"Summary table saved to: {summary_table_path}\")\n",
    "\n",
    "unique_casts = data['cast'].unique()\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(unique_casts)))\n",
    "\n",
    "# Display the summary table\n",
    "print(\"Statistics per cast:\")\n",
    "display(summary_table)\n",
    "\n",
    "# Plot the relative PSD for each cast\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "volume_columns_percentage = [\n",
    "    '2.72_%', '3.2_%', '3.78_%', '4.46_%', '5.27_%', \n",
    "    '6.21_%', '7.33_%', '8.65_%', '10.2_%', '12.1_%', \n",
    "    '14.2_%', '16.8_%', '19.8_%', '23.4_%', '27.6_%', \n",
    "    '32.5_%', '38.4_%', '45.3_%', '53.5_%', '63.1_%', \n",
    "    '74.5_%', '87.9_%', '104_%', '122_%', '144_%', \n",
    "    '170_%', '201_%', '237_%', '280_%', '331_%', \n",
    "    '390_%', '460_%'\n",
    "]\n",
    "for i, cast in enumerate(unique_casts):\n",
    "    cast_data = data[data['cast'] == cast]\n",
    "    volume_concentration_percentage = cast_data[volume_columns_percentage].mean(axis=0).values\n",
    "    plt.plot(grain_sizes, volume_concentration_percentage, label=f'Cast {cast}', color=colors[i])\n",
    "plt.title(f'LISST-100 {location} {campaign_code}: PSD per cast')\n",
    "plt.xlabel('Particle size (µm)')\n",
    "plt.ylabel('Relative volume (%)')\n",
    "plt.legend(title='Relative PSD per cast')\n",
    "plt.grid(axis='both', which='both', linewidth=0.3)\n",
    "plt.xscale('log') \n",
    "plt.tight_layout()\n",
    "psd_plot_path = f\"{output_directory}/{campaign_code}-{location}-LISST100-PSD-per-cast.png\"\n",
    "plt.savefig(psd_plot_path, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot comparison of different parameters between casts\n",
    "parameters = ['mean_diameter_um', 'D10_um', 'D50_um', 'D90_um', 'span', 'mode_um']\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, param in enumerate(parameters):\n",
    "    plt.subplot(2, 3, i + 1) \n",
    "    sns.boxplot(data=data, x='cast', y=param)\n",
    "    plt.title(f'Boxplot of {param}')\n",
    "    plt.xlabel('Cast')\n",
    "    plt.ylabel(param)\n",
    "    plt.grid(axis='both', which='both', linewidth=0.3)\n",
    "plt.tight_layout()\n",
    "psd_plot2_path = f\"{output_directory}/{campaign_code}-{location}-LISST100-cast-comparison.png\"\n",
    "plt.savefig(psd_plot2_path, bbox_inches='tight')\n",
    "plt.show()\n",
    "                                                        ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
