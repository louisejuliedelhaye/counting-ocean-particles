{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c71cbc",
   "metadata": {},
   "source": [
    "# LISST-200x on rosette - data processing notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e947d15",
   "metadata": {},
   "source": [
    "**Disclaimer: this code is under development and can still contain multiple errors.**\n",
    "\n",
    "The following notebook was developed by the Suspended Material and Seabed Monitoring and Modelling (SUMO) group from the Institute of Natural Sciences, Belgium. It aims at providing a standard protocol to process data collected *in-situ* with a LISST-200x instrument placed on a rosette profiling the water column. To use this notebook, simply import the data previously converted to csv by the LISST-200x software, run the cells containing code and inspect the results obtained. The graphs can then be saved as jpg and the processed data as csv.\n",
    "\n",
    "In this notebook, the LISST-200x data processing has been divided into three stages:\n",
    "1. *CSV data generation*: The first stage, during which the raw data is converted from .rbn to .csv, requires the use of the LISST-200x software. \n",
    "2. *Data import & flagging*: This notebook can then be used to carry out the second stage, which includes the import and pre-processing of the data (i.e. date conversion, data flagging). At the end of this stage, the user can save the full dataset with a flagging system and dates as datetime. \n",
    "3. *Statistics computation & visualization*: The third stage performs the calculation of different statistical parameters on the clean dataset (after filtrating data above a certain flag value), such as mean particle diameter, D10, D50, D90 and distribution characteristics, and displays outputs as graphs. The clean dataset as well as the graphs can be saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3604c",
   "metadata": {},
   "source": [
    "**Important!** Before starting the processing, make sure that all the necessary packages and libraries are installed on your computer and that you run the cell below to import everything that is required. Before running this cell, make sure you've installed all the necessary packages on your computer. To do that, you can run the following line in the prompt (we highly recommend to create a new environment to do so):\n",
    "- pip install tk pandas numpy matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b0cab00",
   "metadata": {},
   "source": [
    "                                                    ## Cell 01 ##\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import shutil\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from tkinter.filedialog import asksaveasfilename\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks, savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import colorsys\n",
    "import seaborn as sns\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"paper\")\n",
    "                                                        ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f5c57fbf",
   "metadata": {},
   "source": [
    "## Stage 1: Conversion of rbn files to csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16150704",
   "metadata": {},
   "source": [
    "In this stage, the conversion of the raw LISST-200x data is performed using the LISST-200x software, which can be downloaded freely on https://www.sequoiasci.com/product/lisst-200x/. Always make sure to double-check if the background is acceptable before processing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d821fc3",
   "metadata": {},
   "source": [
    "## Stage 2: Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fbf22",
   "metadata": {},
   "source": [
    "#### Stage 2 - Step 1 : Enter metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81d228",
   "metadata": {},
   "source": [
    "In this cell, the user will be asked to enter the campaign identification code, year at which the data were collected, the name of the sampling location and coordinates in decimal degrees (XX.XXXXX). Press enter after each input.\n",
    "**Important!** Avoid using \"/\" in the campaign identification code, as it might create issues when saving the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3d1b375",
   "metadata": {},
   "source": [
    "                                                    ## Cell 02 ##\n",
    "campaign_code = input(\"Enter the campaign identification code: \")\n",
    "location = input(\"Enter the name of the sampling location: \")\n",
    "latitude = input(\"Enter the latitude of the data: \")\n",
    "longitude = input(\"Enter the longitude of the data: \")\n",
    "                                                        ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aacf1367",
   "metadata": {},
   "source": [
    "#### Stage 2 - Step 2 : Data import & formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46108881",
   "metadata": {},
   "source": [
    "Upon running this cell, a new window should automatically open and invite the user to select the csv file to be processed. **!This window might open in the background and could be hidden under the current window.** Once imported, the data will be attributed column names, campaign identification number and sampling location (as inserted by the user in the cell above). In addition to that, year, day, hour, minute and second columns will be converted to jday and datetime values. A new column containing a flag value (set by default to zero at this step) will be added at the end of the dataset. \n",
    "\n",
    "The dataset will be displayed upon running the cell, enabling the user to check that columns and values were assigned correctly and that the calculated date and time of the data correspond to the actual sampling time."
   ]
  },
  {
   "cell_type": "code",
   "id": "8373efed",
   "metadata": {},
   "source": [
    "                                                    ## Cell 03 ##\n",
    "# Open a window to select input file\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "file_path = filedialog.askopenfilename(title=\"Select a CSV file\", filetypes=[(\"CSV files\", \"*.csv\")])\n",
    "directory_path = os.path.dirname(file_path)\n",
    "\n",
    "output_directory = f\"{directory_path}/LISST-200x-{campaign_code}-{location}-processed\"\n",
    "if os.path.exists(output_directory):\n",
    "    shutil.rmtree(output_directory)\n",
    "    print(f'Existing output folder removed at: {output_directory}')\n",
    "os.makedirs(output_directory)\n",
    "print(f'Output folder created at: {output_directory}')\n",
    "\n",
    "if file_path:  \n",
    "    try:\n",
    "        data = pd.read_csv(file_path, header=None, sep=\",\")\n",
    "        print(\"File successfully loaded!\") \n",
    "        # Add column names\n",
    "        data.columns = [\"1.21\",\"1.6\",\"1.89\",\"2.23\",\"2.63\",\"3.11\",\"3.67\",\"4.33\",\"5.11\",\"6.03\",\"7.11\",\"8.39\",\"9.90\",\"11.7\",\"13.8\",\"16.3\",\"19.2\",\"22.7\",\"26.7\",\"31.6\",\"37.2\",\"43.9\",\"51.9\",\"61.2\",\"72.2\",\"85.2\",\"101\",\"119\",\"140\",\"165\",\"195\",\"230\",\"273\",\"324\",\"386\",\"459\",\"laser_transmission_sensor_mW\",\"supply_voltage_V\",\"external_input_1_V\",\"laser_reference_sensor_mW\",\"depth_in_m\",\"temperature_C\",\"year\",\"month\",\"day\",\"hour\",\"minute\",\"second\",\"external_input_2_V\",\"mean_diameter_um\",\"total_volume_concentration_ppm\",\"relative_humidity_%\",\"accelerometer_x\",\"accelerometer_y\",\"accelerometer_z\",\"raw_pressure_most_significant\",\"raw_pressure_least_significant\",\"ambient_light_counts\",\"external_analog_input_3_V\",\"computed_optical_transmission\",\"beam_attenuation_m\"]\n",
    "        # Add campaign identification number, sampling location name, latitude and longitude\n",
    "        data['campaign'] = campaign_code\n",
    "        data['location'] = location\n",
    "        data['latitude'] = latitude\n",
    "        data['longitude'] = longitude\n",
    "        for col in data.columns.difference(['campaign', 'location']):\n",
    "            if pd.api.types.is_numeric_dtype(data[col]) or data[col].dtype == object:\n",
    "                data[col] = data[col].astype(str).str.replace(r\"[^\\d.-]\", \"\", regex=True)  # Retain numbers, decimal points, and negatives\n",
    "                data[col] = pd.to_numeric(data[col], errors='coerce')  # Convert to numeric, invalid entries to NaN\n",
    "        # Calculate julian day & datetime\n",
    "        data['year'] = data['year'].astype(str).str.zfill(2)\n",
    "        data['month'] = data['month'].astype(str).str.zfill(2)\n",
    "        data['day'] = data['day'].astype(str).str.zfill(2)\n",
    "        data['hour'] = data['hour'].astype(str).str.zfill(2)\n",
    "        data['minute'] = data['minute'].astype(str).str.zfill(2)\n",
    "        data['second'] = data['second'].astype(str).str.zfill(2)\n",
    "        data['datetime'] = data['year'].astype(str) +\"-\"+ data['month'].astype(str) +\"-\"+ data['day'].astype(str)+\" \"+data['hour'].astype(str) +\":\"+ data['minute'].astype(str) +\":\"+ data['second'].astype(str)\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "        data['jday'] = data['datetime'].dt.dayofyear + \\\n",
    "               (data['datetime'].dt.hour / 24) + \\\n",
    "               (data['datetime'].dt.minute / 1440) + \\\n",
    "               (data['datetime'].dt.second / 86400)\n",
    "        data['flag'] = 0\n",
    "        # Compute sampling start and stop datetime\n",
    "        sampling_start = data['datetime'].min()\n",
    "        sampling_stop = data['datetime'].max()\n",
    "        # Display output\n",
    "        print(f'Data were collected at {location} between {sampling_start} and {sampling_stop} during deployment {campaign_code}.')\n",
    "        display(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "else:\n",
    "    print(\"No file selected\")\n",
    "                                                        ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c6a03de",
   "metadata": {},
   "source": [
    "#### Stage 2 - Step 3: Flagging of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008bcbc",
   "metadata": {},
   "source": [
    "In this cell, flagging operations are performed as follows:\n",
    "1. A first check is done on the **reference laser**, if values are below 0.2 mW, this indicates that the laser is likely not working properly. Quality flags will be assigned to 1 if data are above this value and to 3 if not (see flags meaning below). If more than 10% of the data present such a low value, a warning message will be displayed.  \n",
    "2. Data out of the water are flagged with the value of 4 (rows where the beam attenuation is equal to zero). The **beam attenuation** corresponds to the loss of light intensity when a laser beam passes through water, due to both absorption and scattering. Higher attenuation values typically indicate more turbid water. \n",
    "3. The **laser transmission** values are checked. This indicates how much of the laser light has passed through water without being absorbed or scattered. A value of 100% indicates that there has been no light loss through the sample (either water without particles or air) while values below indicate scattering and absorption (which is expected in seawater). In that regard, values below 75% are flagged 4. Values above 100%, however, indicate a sensor malfunction or miscalibration. If most data (>50%) are above 100%, then a warning message is generated. Data above 100 are flagged with the value of 3.\n",
    "4. The **optical transmission** is checked: values should be ranging between 0 and 1. Values outside that range should be discarded (flag 4). However, values above 0.98-0.995 reflect extremely clear water conditions, meaning a low signal-to-noise ratio (flag 3, to be taken with caution. Above that range, values should be discarded), whereas values below 0.10 reflect very turbid data and should be discarded. \n",
    "5. Finally, a flag of 3 is attributed to **outliers** detected based on the total volume concentration, depth, optical beam transmission and temperature and comparing the value of each point to the calculated mean over a moving window of 25 points. Values are considered outliers when they are higher or lower than the moving average plus or minus three times the standard deviation.\n",
    "\n",
    "Once the flagging has been performed, a graph allows the user to visualize the quality of the data. The complete flagged dataset is automatically saved in the output directory selected by the user at the beginning of this notebook. In stage 3, a cell then allows to filter out all the data with a quality flag equal or higher than 4.\n",
    "\n",
    "Quality flags are defined following the quality flags standards defined by the NERC Environmental Data Service of the British Oceanographic Data Centre (https://vocab.nerc.ac.uk/collection/L20/current/):\n",
    "- 0: No quality control\n",
    "- 1: Good value\n",
    "- 2: Probably good value\n",
    "- 3: Probably bad value\n",
    "- 4: Bad value\n",
    "- 5: Changed value\n",
    "- 6: Value below detection\n",
    "- 7: Value in excess\n",
    "- 8: Interpolated value\n",
    "- 9: Missing value"
   ]
  },
  {
   "cell_type": "code",
   "id": "e25e1227",
   "metadata": {},
   "source": [
    "                                                    ## Cell 04 ##\n",
    "# Step 1: Flagging based on reference_laser\n",
    "data['flag'] = data.apply(lambda row: max(row['flag'], 1) if row['laser_reference_sensor_mW'] > 0.02 else max(row['flag'], 3), axis=1)\n",
    "reference_below_thsld = (data['laser_reference_sensor_mW'] <= 0.02).sum() / len(data) * 100\n",
    "if reference_below_thsld > 20:  # If more than 20% of the data are below 0.02 mW\n",
    "    messagebox.showwarning(\"Laser should be checked\", \n",
    "                           f\"Warning: {reference_below_thsld:.0f}% of the data have a laser reference value below 0.02 mW. \"\n",
    "                           \"Please check laser.\")\n",
    "\n",
    "# Step 2: Beam attenuation flagging\n",
    "data['flag'] = data.apply(lambda row: max(row['flag'], 4) if row['beam_attenuation_m'] <= 0 else row['flag'], axis=1)\n",
    "\n",
    "# Step 3: Laser transmission flagging\n",
    "data['flag'] = data.apply(lambda row: 4 if row['laser_transmission_sensor_mW'] < 0.75 else row['flag'], axis=1)\n",
    "underwater_data = data[data['beam_attenuation_m'] > 0]\n",
    "above_100_percent = (underwater_data['laser_transmission_sensor_mW'] > 1).sum() / len(underwater_data) * 100\n",
    "above_100_percent = (data['laser_transmission_sensor_mW'] > 1).sum() / len(data) * 100\n",
    "if above_100_percent > 50:  # If more than 50% of the data are above 100%\n",
    "    messagebox.showwarning(\"Recalibration Needed\", \n",
    "                           f\"Warning: {above_100_percent:.0f}% of the underwater data are above 100% transmission. \"\n",
    "                           \"Please recalibrate the sensor.\")\n",
    "    \n",
    "# Step 4: Optical transmission flagging\n",
    "data['flag'] = data.apply(lambda row: 3 if row['computed_optical_transmission'] > 0.98 and row['computed_optical_transmission'] < 0.995 and row['flag'] < 3 else row['flag'], axis=1)\n",
    "data['flag'] = data.apply(lambda row: 4 if row['computed_optical_transmission'] >= 0.995 and row['flag'] < 4 else row['flag'], axis=1)\n",
    "data['flag'] = data.apply(lambda row: 4 if row['computed_optical_transmission'] <= 0.10 and row['flag'] < 4 else row['flag'], axis=1)\n",
    "    \n",
    "# Step 5: Outlier detection using rolling mean and standard deviation\n",
    "columns_to_check = ['total_volume_concentration_ppm', 'temperature_C', 'depth_in_m', 'computed_optical_transmission']\n",
    "window_size = 25  # Define the window size for rolling calculations\n",
    "threshold = 3  # Define the threshold for outlier detection\n",
    "\n",
    "for col in columns_to_check:\n",
    "    rolling_mean = data[col].rolling(window=window_size, center=True).mean()\n",
    "    rolling_std = data[col].rolling(window=window_size, center=True).std()\n",
    "    outlier_column = f'is_outlier_{col}'  # Create a column to flag outliers for each variable\n",
    "    data[outlier_column] = abs(data[col] - rolling_mean) > (threshold * rolling_std)\n",
    "\n",
    "    # Update the 'flag' column if the data is an outlier\n",
    "    data['flag'] = data.apply(\n",
    "        lambda row: 3 if row[outlier_column] else row['flag'], axis=1\n",
    "    )\n",
    "\n",
    "# Combine all outlier flags into one column for visualization\n",
    "data['is_outlier'] = data[[f'is_outlier_{col}' for col in columns_to_check]].any(axis=1)\n",
    "\n",
    "# Count total outliers\n",
    "outlier_count = data['is_outlier'].sum()\n",
    "print(f'Number of outliers detected: {outlier_count}')\n",
    "\n",
    "# Warning message if quality flags exceed 75%\n",
    "percentage_high_flags = (data['flag'] >= 3).sum() / len(data) * 100\n",
    "if percentage_high_flags > 75:  # If more than 75% of the data have a quality flag of 3 or higher\n",
    "    messagebox.showwarning(\"Unsatisfactory data quality\", \n",
    "                           f\"Warning: {percentage_high_flags:.0f}% of the data have a quality flag above 3. \"\n",
    "                           \"Data should be used with caution.\")\n",
    "\n",
    "# Output\n",
    "print('Flagging has been successfully performed on the complete dataset:')\n",
    "display(data)\n",
    "\n",
    "# Visualizing outliers for each column\n",
    "for col in columns_to_check:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(data.index, data[col], label='', color='blue', s=10, alpha=0.5)\n",
    "    plt.scatter(\n",
    "        data[data[f'is_outlier_{col}']].index, \n",
    "        data[data[f'is_outlier_{col}']][col], \n",
    "        label='Outliers', color='red', marker='x', s=15\n",
    "    )\n",
    "    plt.grid(axis='both', which='both', linewidth=0.3)\n",
    "    plt.title(f'Outlier detection for {col}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(col.replace('_', ' ').title())\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizing flags\n",
    "flag_counts = data['flag'].value_counts().sort_index()\n",
    "colors = {1: 'green', 2: 'yellow', 3: 'orange', 4: 'red'}\n",
    "fig, ax = plt.subplots()\n",
    "flag_counts.plot(kind='bar', color=[colors.get(flag, 'blue') for flag in flag_counts.index], ax=ax)\n",
    "ax.grid(axis='both', which='both', linewidth=0.3)\n",
    "ax.set_title(\"Count per flag category\")\n",
    "ax.set_xlabel(\"Flag\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Save as csv\n",
    "data_path = f\"{output_directory}/{campaign_code}-{location}-LISST200x-full-data.csv\"\n",
    "if data_path:\n",
    "    data.to_csv(data_path, index=False)\n",
    "    print(f\"File saved to {file_path}\")\n",
    "else:\n",
    "    print(\"Save operation cancelled.\")\n",
    "                                                        ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6f6156e4",
   "metadata": {},
   "source": [
    "## Stage 3: Statistics computation & visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e8a0d",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 1: Statistics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653d32c",
   "metadata": {},
   "source": [
    "By running this cell, a series of statistics are calculated on the full dataset. This includes:\n",
    "1. **Total volume concentration**: This sums up the volume concentration of each grain size class per row.\n",
    "2. **Relative volume concentration**: This calculates the percentage that each class represents compared to the total volume concentration.\n",
    "3. **Mean diameter**: This corresponds to the mean diameter of particles in each row weighted by their volume concentration and normalized by the total volume concentration.\n",
    "4. **D10, D50, D90 values**: These represent the diameter below which 10%, 50% or 90% of the volume of the data are found. The D10 helps characterizing the finer fraction of the sample, the D50 corresponds to the median diameter and the D90 characterizes the coarser sediment fraction. These values are calculated based on the cumulative distribution.\n",
    "5. **Span**: It is a measure of the sorting of particle sizes, as a normalized measure of the distribution spread around the median particle size, showing the relative range of the middle 80% of the particle size distribution. Small values indicate good sorting (close to 1) while a larger span indicates poor sorting of the particles.\n",
    "6. **Standard deviation**: It measures the average dispersion of particle sizes from the mean, a greater standard deviation shows greater variability around the mean with possible outliers or tails. If the span and standard deviation are low, it shows a uniform distribution; if the span is moderate but the standard deviation is high, it shows a moderate spread with outliers; and if the span and standard deviation are high, it shows a wide distribution.\n",
    "7. **Mode**: The mode is the particle size that has the largest volume of the distribution (peak in the distribution).\n",
    "8. **Peaks in the distribution**: It identifies peaks in the particle size distribution.\n",
    "\n",
    "The particle size distribution combined with the cumulative volume concentration is displayed upon running of this cell as well as a table with the mean value of all the calculated parameters. The graph and updated complete clean dataframe are automatically saved in the directory selected by the user."
   ]
  },
  {
   "cell_type": "code",
   "id": "82582c42",
   "metadata": {},
   "source": [
    "                                                    ## Cell 05 ##  \n",
    "# Check if any row has a total volume concentration of zero\n",
    "zero_volume_rows = data[data['total_volume_concentration_ppm'] == 0]\n",
    "print(f\"{len(zero_volume_rows)} on {len(data)} rows with zero total volume concentration will be removed.\")\n",
    "display(zero_volume_rows)\n",
    "data = data[data['total_volume_concentration_ppm'] != 0].reset_index(drop=True)\n",
    "\n",
    "# Calculate the total volume concentration\n",
    "volume_concentration_columns = [\"1.21\",\"1.6\",\"1.89\",\"2.23\",\"2.63\",\"3.11\",\"3.67\",\"4.33\",\"5.11\",\"6.03\",\"7.11\",\"8.39\",\"9.90\",\"11.7\",\"13.8\",\"16.3\",\"19.2\",\"22.7\",\"26.7\",\"31.6\",\"37.2\",\"43.9\",\"51.9\",\"61.2\",\"72.2\",\"85.2\",\"101\",\"119\",\"140\",\"165\",\"195\",\"230\",\"273\",\"324\",\"386\",\"459\"]\n",
    "average_volume_fractions = data[volume_concentration_columns].mean()\n",
    "volume_concentrations = data[volume_concentration_columns].values\n",
    "grain_sizes = np.array([float(col) for col in volume_concentration_columns])\n",
    "\n",
    "# Calculate the percentage of each class compared to the total volume concentration\n",
    "for col in volume_concentration_columns:\n",
    "    data[f'{col}_%'] = ((data[col] / data['total_volume_concentration_ppm']) * 100).astype(float)\n",
    "\n",
    "# Cumulative volume concentration\n",
    "cumulative_volumes = np.cumsum(data[volume_concentration_columns].values, axis=1)\n",
    "cumulative_volumes /= cumulative_volumes[:, -1][:, None]\n",
    "\n",
    "# Calculating percentiles based on the slope of the cumulative distribution\n",
    "def calculate_d_percentile(cumulative_volumes, grain_sizes, percentile):\n",
    "    target = percentile / 100.0\n",
    "    dx_values = []\n",
    "\n",
    "    for sample in cumulative_volumes:\n",
    "        if np.all(sample == 0):\n",
    "            dx_values.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        idx = np.searchsorted(sample, target)\n",
    "\n",
    "        if idx == 0:\n",
    "            dx_values.append(grain_sizes[0])\n",
    "        elif idx >= len(sample):\n",
    "            dx_values.append(grain_sizes[-1])\n",
    "        else:\n",
    "            x0, x1 = grain_sizes[idx - 1], grain_sizes[idx]\n",
    "            y0, y1 = sample[idx - 1], sample[idx]\n",
    "            dx = x0 + ((target - y0) / (y1 - y0)) * (x1 - x0)\n",
    "            dx_values.append(dx)\n",
    "\n",
    "    return np.array(dx_values)\n",
    "\n",
    "data['D10_um'] = calculate_d_percentile(cumulative_volumes, grain_sizes, 10)\n",
    "data['D50_um'] = calculate_d_percentile(cumulative_volumes, grain_sizes, 50)\n",
    "data['D90_um'] = calculate_d_percentile(cumulative_volumes, grain_sizes, 90)\n",
    "\n",
    "# Calculate the span\n",
    "data['span'] = (data['D90_um'] - data['D10_um']) / data['D50_um']\n",
    "\n",
    "# Calculate the standard deviation\n",
    "def calculate_std(grain_sizes, volume_concentrations, mean_diameter):\n",
    "    variance = np.sum(volume_concentrations * (grain_sizes - mean_diameter[:, None])**2, axis=1) / np.sum(volume_concentrations, axis=1)\n",
    "    return np.sqrt(variance)\n",
    "data['std_dev_um'] = calculate_std(grain_sizes, volume_concentrations, data['mean_diameter_um'].values)\n",
    "\n",
    "# Calculate the mode\n",
    "def calculate_mode(grain_sizes, volume_concentrations):\n",
    "    mode_values = []\n",
    "    for vc in volume_concentrations:\n",
    "        if np.all(vc == 0):\n",
    "            mode_values.append(np.nan)\n",
    "        else:\n",
    "            mode_index = np.argmax(vc)\n",
    "            mode_values.append(grain_sizes[mode_index])\n",
    "    return np.array(mode_values)\n",
    "data['mode_um'] = calculate_mode(grain_sizes, volume_concentrations)\n",
    "\n",
    "# Identify peaks in the distribution \n",
    "def find_all_peaks(grain_sizes, volume_concentrations):\n",
    "    all_peaks = []\n",
    "    for vc in volume_concentrations:\n",
    "        if np.all(vc == 0):\n",
    "            all_peaks.append([])\n",
    "        else:\n",
    "            peaks, _ = find_peaks(vc)\n",
    "            peak_sizes = grain_sizes[peaks]\n",
    "            all_peaks.append(peak_sizes.tolist())\n",
    "    return all_peaks\n",
    "data['peaks'] = find_all_peaks(grain_sizes, volume_concentrations)\n",
    "\n",
    "# Display updated dataframe\n",
    "print('Statistics have been successfully computed')\n",
    "display(data)\n",
    "\n",
    "# Display the histogram of the particle size distribution and the cumulative volume distribution (in red)\n",
    "total_volume_concentration_per_class = data[volume_concentration_columns].sum(axis=0)\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6), dpi=300)\n",
    "ax1.plot(grain_sizes, total_volume_concentration_per_class, color='blue', marker='o', \n",
    "         label='Particle size distribution', linestyle='-')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Grain size (µm)')\n",
    "ax1.set_ylabel('Total volume concentration (µl/l)')\n",
    "ax1.set_title(f'LISST-200x {location} {campaign_code}')\n",
    "ax1.grid(axis='both', which='both', linewidth=0.3)\n",
    "cumulative_volumes = np.cumsum(total_volume_concentration_per_class)\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(grain_sizes, cumulative_volumes / cumulative_volumes[-1] * 100, color='red', \n",
    "         label='Cumulative distribution', marker='o')\n",
    "ax2.set_ylabel('Cumulative volume (%)', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax1.axvline(data['D10_um'].mean(), color='green', linestyle='--', label='D10')\n",
    "ax1.axvline(data['D50_um'].mean(), color='orange', linestyle='--', label='D50')\n",
    "ax1.axvline(data['D90_um'].mean(), color='purple', linestyle='--', label='D90')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "graph_file_path = f\"{output_directory}/{campaign_code}-{location}-LISST200x-full-PSD.png\"\n",
    "fig.savefig(graph_file_path) \n",
    "\n",
    "# Display table with the mean of each parameter\n",
    "mean_values = {\n",
    "    'Total volume concentration (ppm)': data['total_volume_concentration_ppm'].mean(),\n",
    "    'Mean diameter (µm)': data['mean_diameter_um'].mean(),\n",
    "    'D10 (µm)': data['D10_um'].mean(),\n",
    "    'D50 (µm)': data['D50_um'].mean(),\n",
    "    'D90 (µm)': data['D90_um'].mean(),\n",
    "    'Span': data['span'].mean(),\n",
    "    'Standard deviation (µm)': data['std_dev_um'].mean(),\n",
    "    'Mode (µm)': data['mode_um'].mean()\n",
    "}\n",
    "mean_values_df = pd.DataFrame(mean_values, index=[0])\n",
    "print('Mean values of calculated parameters for the complete clean dataset:')\n",
    "display(mean_values_df)\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 12), dpi=300, sharex=True)\n",
    "axs[0].set_title(f'LISST-200x {location} {campaign_code}')\n",
    "axs[0].plot(data['datetime'], data['depth_in_m'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[0].set_ylabel('Depth (m)')\n",
    "axs[0].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[1].plot(data['datetime'], data['computed_optical_transmission'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[1].set_ylabel('Optical transmission (%)')\n",
    "axs[1].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[2].plot(data['datetime'], data['temperature_C'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[2].set_ylabel('Temperature (°C)')\n",
    "axs[2].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[3].plot(data['datetime'], data['total_volume_concentration_ppm'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[3].set_ylabel('Total volume concentration (ppm)')\n",
    "axs[3].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[4].plot(data['datetime'], data['mean_diameter_um'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[4].set_ylabel('Mean diameter (µm)')\n",
    "axs[4].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[4].set_xlabel('')\n",
    "axs[4].xaxis.set_major_formatter(mdates.DateFormatter('%D'))\n",
    "axs[4].xaxis.set_major_locator(mdates.DayLocator(interval=7))\n",
    "fig.align_ylabels()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "graph_file_path = f\"{output_directory}/{campaign_code}-{location}-LISST200x-g-full-graph2.png\"\n",
    "fig.savefig(graph_file_path) \n",
    "\n",
    "# Save output\n",
    "print(f'Graphs are saved to: {output_directory}')\n",
    "                                                        ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af76a71b",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 2: Manual adjustment of acceptable optical transmission, total volume concentration and/or mean diameter values (optional, upon verification on the graph above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d87d5",
   "metadata": {},
   "source": [
    "The following cell is optional and does not have to be ran if no additional filtering of the data is needed. This decision should be done following a visual inspection of the graph generated in cell 5 above. The user can define minimum and/or maximum thresholds for five possible fields (optical transmission, total volume concentration, mean diameter, pressure and/or temperature) below and/or above which the data are flagged to 4. The full dataset is saved again.\n",
    "If one threshold wants to be ignored, it can be set to -99.99."
   ]
  },
  {
   "cell_type": "code",
   "id": "2d0eb9a9",
   "metadata": {},
   "source": [
    "                                                    ## Cell 06 ##\n",
    "# Initialize thresholds\n",
    "minimum_threshold = -99.99\n",
    "maximum_threshold = -99.99\n",
    "\n",
    "# Dropdown for field selection\n",
    "field_threshold_dropdown = widgets.Dropdown(\n",
    "    options=['optical transmission', 'total volume concentration', 'mean diameter', 'depth', 'temperature'],\n",
    "    value=None,\n",
    "    description='Select field on which to apply a threshold:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Widgets for threshold inputs (initially hidden)\n",
    "min_threshold_widget = widgets.FloatText(\n",
    "    value=-99.99,\n",
    "    description='Minimum threshold:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "max_threshold_widget = widgets.FloatText(\n",
    "    value=-99.99,\n",
    "    description='Maximum threshold:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Button to apply thresholds (initially hidden)\n",
    "apply_button = widgets.Button(\n",
    "    description='Apply thresholds',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "# Container to display widgets dynamically\n",
    "threshold_widgets_container = widgets.VBox([])\n",
    "\n",
    "# Function to handle dropdown changes\n",
    "def on_field_change(change):\n",
    "    selected_field = change['new']\n",
    "    if selected_field:\n",
    "        print(f\"Selected field: {selected_field}\")\n",
    "        # Display threshold widgets and apply button\n",
    "        threshold_widgets_container.children = [min_threshold_widget, max_threshold_widget, apply_button]\n",
    "    else:\n",
    "        # Hide threshold widgets and apply button\n",
    "        threshold_widgets_container.children = []\n",
    "\n",
    "# Observe dropdown changes\n",
    "field_threshold_dropdown.observe(on_field_change, names='value')\n",
    "\n",
    "# Function to apply thresholds\n",
    "def apply_thresholds(_):\n",
    "    global minimum_threshold, maximum_threshold\n",
    "    minimum_threshold = min_threshold_widget.value\n",
    "    maximum_threshold = max_threshold_widget.value\n",
    "    print(f\"Applying thresholds: Min = {minimum_threshold}, Max = {maximum_threshold}, Field = {field_threshold_dropdown.value}\")\n",
    "    \n",
    "    # Apply flags based on selected field\n",
    "    if field_threshold_dropdown.value == 'optical transmission':\n",
    "        if minimum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['computed_optical_transmission'] < minimum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "        if maximum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['computed_optical_transmission'] > maximum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "    elif field_threshold_dropdown.value == 'total volume concentration':\n",
    "        if minimum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['total_volume_concentration_ppm'] < minimum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "        if maximum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['total_volume_concentration_ppm'] > maximum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "    elif field_threshold_dropdown.value == 'mean diameter':\n",
    "        if minimum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['mean_diameter_um'] < minimum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "        if maximum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['mean_diameter_um'] > maximum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "    elif field_threshold_dropdown.value == 'depth':\n",
    "        if minimum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['depth_in_m'] < minimum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "        if maximum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['depth_in_m'] > maximum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "    elif field_threshold_dropdown.value == 'temperature':\n",
    "        if minimum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['temperature_C'] < minimum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "        if maximum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['temperature_C'] > maximum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "\n",
    "    # Save updated data\n",
    "    data_path = f\"{output_directory}/{deployment_code}-{location}-LISST200x-full-data.csv\"\n",
    "    data.to_csv(data_path, index=False)\n",
    "    print(f\"File saved to {data_path}\")\n",
    "    print(f\"Flagging has been successfully performed and the updated dataset has been saved to: {output_directory}\")\n",
    "\n",
    "# Connect button to function\n",
    "apply_button.on_click(apply_thresholds)\n",
    "\n",
    "# Display dropdown and container\n",
    "display(field_threshold_dropdown, threshold_widgets_container)\n",
    "                                                        ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b09a2e9c",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 3: Removal of suspicious data & creation of a 'clean' dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc35311",
   "metadata": {},
   "source": [
    "This step allows the user to choose a certain flag threshold at and above which data are discarded for further analysis. The default value is set to 4, meaning all data flagged 4 and above will be removed from the filtered dataframe. In this cell, values considered outliers can also be removed."
   ]
  },
  {
   "cell_type": "code",
   "id": "ad09a253",
   "metadata": {},
   "source": [
    "                                                    ## Cell 07 ##\n",
    "try:\n",
    "    threshold = int(input(\"Enter the threshold for flag filtration (default is 4): \") or 4)\n",
    "except ValueError:\n",
    "    print(\"Invalid input, defaulting to threshold = 4.\")\n",
    "    threshold = 4\n",
    "\n",
    "outliers = input(\"Remove outliers (yes or no): \")\n",
    "\n",
    "# Apply initial filters\n",
    "filtered_data = data[\n",
    "    (data['total_volume_concentration_ppm'] != 0) & \n",
    "    (data['mean_diameter_um'].notna()) & \n",
    "    (data['flag'] < threshold)\n",
    "]\n",
    "\n",
    "# Additional filtering based on outliers\n",
    "if outliers.lower() == 'yes':\n",
    "    filtered_data = filtered_data[filtered_data['is_outlier'] != True]\n",
    "elif outliers.lower() != 'no':\n",
    "    print(\"Invalid input for outliers; no outlier filtering applied.\")\n",
    "\n",
    "# Display the filtered data\n",
    "print(f\"After filtering, the dataset contains {len(filtered_data)} rows:\")\n",
    "display(filtered_data)\n",
    "\n",
    "# Save filtered data as csv\n",
    "filtered_data_path = f\"{output_directory}/{campaign_code}-{location}-LISST200x-clean-data.csv\"\n",
    "filtered_data.to_csv(filtered_data_path, index=False)\n",
    "print(f\"Filtered data saved to {filtered_data_path}\")\n",
    "\n",
    "# Apply Savitzky-Golay filter\n",
    "filtered_data['mean_diameter_filtered_6h'] = savgol_filter(filtered_data['mean_diameter_um'], 359, 3)\n",
    "filtered_data['mean_diameter_filtered_12h'] = savgol_filter(filtered_data['mean_diameter_um'], 719, 3)\n",
    "filtered_data['mean_diameter_filtered_24h'] = savgol_filter(filtered_data['mean_diameter_um'], 1439, 3)\n",
    "filtered_data['total_volume_filtered_6h'] = savgol_filter(filtered_data['total_volume_concentration_ppm'], 359, 3)\n",
    "filtered_data['total_volume_filtered_12h'] = savgol_filter(filtered_data['total_volume_concentration_ppm'], 719, 3)\n",
    "filtered_data['total_volume_filtered_24h'] = savgol_filter(filtered_data['total_volume_concentration_ppm'], 1439, 3)\n",
    "\n",
    "# New graph with clean and filtered data\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 12), dpi=300, sharex=True)\n",
    "axs[0].set_title(f'LISST-200x {location} {campaign_code}')\n",
    "axs[0].plot(filtered_data['datetime'], filtered_data['depth_in_m'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[0].set_ylabel('Depth (m)')\n",
    "axs[0].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[1].plot(filtered_data['datetime'], filtered_data['computed_optical_transmission'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[1].set_ylabel('Optical transmission (%)')\n",
    "axs[1].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[2].plot(filtered_data['datetime'], filtered_data['temperature_C'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[2].set_ylabel('Temperature (°C)')\n",
    "axs[2].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[3].plot(filtered_data['datetime'], filtered_data['total_volume_concentration_ppm'], color='steelblue', linestyle='-', linewidth=0.4, label='Original')\n",
    "axs[3].plot(filtered_data['datetime'], filtered_data['total_volume_filtered_6h'], color='lightblue', linestyle='-', linewidth=0.6, label='6h')\n",
    "axs[3].plot(filtered_data['datetime'], filtered_data['total_volume_filtered_12h'], color='lime', linestyle='-', linewidth=0.6, label='12h')\n",
    "axs[3].plot(filtered_data['datetime'], filtered_data['total_volume_filtered_24h'], color='red', linestyle='-', linewidth=0.6, label='24h')\n",
    "axs[3].set_ylabel('Total volume concentration (µl/l)')\n",
    "axs[3].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[3].legend()\n",
    "axs[4].plot(filtered_data['datetime'], filtered_data['mean_diameter_um'], color='steelblue', linestyle='-', linewidth=0.4, label='Original')\n",
    "axs[4].plot(filtered_data['datetime'], filtered_data['mean_diameter_filtered_6h'], color='lightblue', linestyle='-', linewidth=0.6, label='6h')\n",
    "axs[4].plot(filtered_data['datetime'], filtered_data['mean_diameter_filtered_12h'], color='lime', linestyle='-', linewidth=0.6, label='12h')\n",
    "axs[4].plot(filtered_data['datetime'], filtered_data['mean_diameter_filtered_24h'], color='red', linestyle='-', linewidth=0.6, label='24h')\n",
    "axs[4].set_ylabel('Mean diameter (µm)')\n",
    "axs[4].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[4].set_xlabel('')\n",
    "axs[4].xaxis.set_major_locator(mdates.DayLocator(interval=7))\n",
    "fig.align_ylabels()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "graph_file_path = f\"{output_directory}/{campaign_code}-{location}-LISST200x-g-filtered-graph2.png\"\n",
    "fig.savefig(graph_file_path)\n",
    "                                                        ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bfa59c69",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 4: Division of the clean dataset into casts"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this cell, the user can choose to separate the casts manually or automatically. If the first option is chosen, the user will be asked to define the expected number of casts and the start and stop datetime of each of them. If the user decides the automatic option, a cast letter will be attributed to each set of consecutive values separated from the previous set by a certain user-defined time interval (in minutes). If the user has other data available for these casts, we advise comparing their timings to the table generated in this cell.",
   "id": "128b4f7a2b060eb1"
  },
  {
   "cell_type": "code",
   "id": "8ec5c600",
   "metadata": {},
   "source": [
    "                                                    ## Cell 08 ##\n",
    "# Ask the user whether to define cast intervals manually or automatically\n",
    "mode = input(\"Would you like to define cast intervals manually or automatically? (Enter 'manual' or 'auto'): \").strip().lower()\n",
    "\n",
    "if mode == 'manual':\n",
    "    # Manual mode: User defines the number of casts and their start/stop times\n",
    "    num_casts = int(input(\"Enter the number of expected casts: \"))\n",
    "    cast_intervals = []\n",
    "    for i in range(num_casts):\n",
    "        print(f\"Cast {chr(65 + i)}:\")\n",
    "        start_time = input(\"Enter the start datetime (YYYY-MM-DD HH:MM:SS): \")\n",
    "        stop_time = input(\"Enter the stop datetime (YYYY-MM-DD HH:MM:SS): \")\n",
    "        try:\n",
    "            start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "            stop_time = datetime.strptime(stop_time, '%Y-%m-%d %H:%M:%S')\n",
    "            cast_intervals.append({'cast': chr(65 + i), 'start': start_time, 'stop': stop_time})\n",
    "        except ValueError:\n",
    "            print(\"Invalid datetime format. Please enter the date and time in 'YYYY-MM-DD HH:MM:SS' format.\")\n",
    "            continue\n",
    "\n",
    "    # Assign casts based on user-defined intervals\n",
    "    cast_numbers = []\n",
    "    for _, row in filtered_data.iterrows():\n",
    "        assigned = False\n",
    "        for interval in cast_intervals:\n",
    "            if interval['start'] <= row['datetime'] <= interval['stop']:\n",
    "                cast_numbers.append(interval['cast'])\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            cast_numbers.append(None)  # No cast assigned\n",
    "\n",
    "    filtered_data = filtered_data.copy()\n",
    "    filtered_data['cast'] = cast_numbers\n",
    "\n",
    "elif mode == 'auto':\n",
    "    # Automatic mode: Use time difference to define casts\n",
    "    cast_interval = int(input(\"Enter the expected cast interval (in minutes): \"))\n",
    "    non_zero_data = filtered_data[filtered_data['total_volume_concentration_ppm'] != 0].copy()\n",
    "    non_zero_data = non_zero_data.sort_values('datetime').reset_index()\n",
    "    current_cast = 'A'\n",
    "    cast_numbers = [current_cast]\n",
    "\n",
    "    # Loop through the dataset and check the time difference between consecutive rows\n",
    "    for i in range(1, len(non_zero_data)):\n",
    "        time_diff = non_zero_data['datetime'].iloc[i] - non_zero_data['datetime'].iloc[i - 1]\n",
    "        if time_diff >= timedelta(minutes=cast_interval):\n",
    "            current_cast = chr(ord(current_cast) + 1)\n",
    "        cast_numbers.append(current_cast)\n",
    "\n",
    "    # Add the cast numbers as a new column in the dataframe\n",
    "    filtered_data = non_zero_data.copy()\n",
    "    filtered_data['cast'] = cast_numbers\n",
    "\n",
    "else:\n",
    "    print(\"Invalid option. Please enter 'manual' or 'auto'.\")\n",
    "\n",
    "# Generate a summary table for both modes\n",
    "if 'cast' in filtered_data.columns:\n",
    "    summary_table = filtered_data.groupby('cast').agg(\n",
    "        datetime_start=('datetime', 'min'),\n",
    "        datetime_stop=('datetime', 'max')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Optional renaming\n",
    "    rename_casts = input(f\"\\n{len(summary_table)} casts found. Would you like to rename the casts manually? (yes or no): \").strip().lower()\n",
    "    if rename_casts == 'yes':\n",
    "        cast_rename_map = {}\n",
    "        for original_cast in summary_table['cast']:\n",
    "            new_name = input(f\"Rename cast '{original_cast}' (press Enter to keep it unchanged): \").strip()\n",
    "            if new_name:\n",
    "                cast_rename_map[original_cast] = new_name\n",
    "\n",
    "        # Apply the renaming to both the filtered_data and the summary table\n",
    "        filtered_data['cast'] = filtered_data['cast'].replace(cast_rename_map)\n",
    "        summary_table['cast'] = summary_table['cast'].replace(cast_rename_map)\n",
    "\n",
    "        # Display the summary table\n",
    "    print(\"\\nSummary of casts timings:\")\n",
    "    display(summary_table)\n",
    "\n",
    "else:\n",
    "    print(\"No cast assignments were made.\")\n",
    "                                                        ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2b438d6",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 5: Comparison of the different casts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This cell generates a table and several graphs comparing the different casts of the processed dataset. These are saved in the directory previously selected by the user, together with a csv containing the mean values of each cast.",
   "id": "31d82962b235d2c1"
  },
  {
   "cell_type": "code",
   "id": "8d52dcf8",
   "metadata": {},
   "source": [
    "                                                    ## Cell 09 ##\n",
    "# Use filtered data\n",
    "data = filtered_data if not filtered_data.empty else data\n",
    "\n",
    "volume_columns_percentage = [\n",
    "    '1.21_%', '1.6_%', '1.89_%', '2.23_%', '2.63_%',\n",
    "    '3.11_%', '3.67_%', '4.33_%', '5.11_%', '6.03_%',\n",
    "    '7.11_%', '8.39_%', '9.90_%', '11.7_%', '13.8_%',\n",
    "    '16.3_%', '19.2_%', '22.7_%', '26.7_%', '31.6_%',\n",
    "    '37.2_%', '43.9_%', '51.9_%', '61.2_%', '72.2_%',\n",
    "    '85.2_%', '101_%', '119_%', '140_%', '165_%',\n",
    "    '195_%', '230_%', '273_%', '324_%', '386_%', '459_%'\n",
    "]\n",
    "\n",
    "# === ADDED: User-defined color gradient groups for casts ===\n",
    "unique_casts = sorted(data['cast'].unique())\n",
    "cast_color_map = {}\n",
    "\n",
    "manual_mode = input(\"Do you want to define custom cast colors? (y/n): \").strip().lower().startswith('y')\n",
    "\n",
    "if manual_mode:\n",
    "    # Number of groups\n",
    "    while True:\n",
    "        try:\n",
    "            num_groups = int(input(\"How many color groups would you like? \"))\n",
    "            if num_groups > 0:\n",
    "                break\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid integer.\")\n",
    "\n",
    "    assigned = []\n",
    "    for gi in range(num_groups):\n",
    "        base = input(f\"Group {gi+1}: enter a base color name/hex (e.g., red or #ff0000): \").strip()\n",
    "        try:\n",
    "            base_rgb = matplotlib.colors.to_rgb(base)\n",
    "        except ValueError:\n",
    "            print(\"Invalid color—using 'grey'.\")\n",
    "            base_rgb = matplotlib.colors.to_rgb('grey')\n",
    "\n",
    "        # Select casts for this group\n",
    "        while True:\n",
    "            sel = input(f\"Enter cast names for this group, comma-separated: \").split(',')\n",
    "            sel = [s.strip() for s in sel if s.strip() in unique_casts and s.strip() not in assigned]\n",
    "            if sel:\n",
    "                assigned += sel\n",
    "                break\n",
    "            print(\"No valid casts—please retry.\")\n",
    "\n",
    "        # Assign gradient\n",
    "        def light(rgb, f):  # lighten by factor f (0–1)\n",
    "            h, l, s = colorsys.rgb_to_hls(*rgb)\n",
    "            return colorsys.hls_to_rgb(h, min(1, l + f*(1-l)), s)\n",
    "\n",
    "        n = len(sel)\n",
    "        for i, c in enumerate(sel):\n",
    "            f = (i / max(n-1, 1)) * 0.5\n",
    "            rgb2 = light(base_rgb, 0.5 - f)\n",
    "            cast_color_map[c] = matplotlib.colors.to_hex(rgb2)\n",
    "\n",
    "    # Default colors for any remaining casts\n",
    "    rem = [c for c in unique_casts if c not in cast_color_map]\n",
    "    if rem:\n",
    "        defaults = plt.cm.viridis(np.linspace(0,1,len(rem)))\n",
    "        for c, clr in zip(rem, defaults):\n",
    "            cast_color_map[c] = matplotlib.colors.to_hex(clr)\n",
    "else:\n",
    "    # Automatic color assignment\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_casts)))\n",
    "    cast_color_map = {c: matplotlib.colors.to_hex(clr) for c, clr in zip(unique_casts, colors)}\n",
    "# === END COLOR SECTION ===\n",
    "\n",
    "# Create summary table per cast...\n",
    "summary_table = data.groupby('cast').agg(\n",
    "    datetime_start=('datetime', 'min'),\n",
    "    datetime_stop=('datetime', 'max'),\n",
    "    mean_total_volume_concentration=('total_volume_concentration_ppm', 'mean'),\n",
    "    mean_diameter=('mean_diameter_um', 'mean'),\n",
    "    mean_D10=('D10_um', 'mean'),\n",
    "    mean_D50=('D50_um', 'mean'),\n",
    "    mean_D90=('D90_um', 'mean'),\n",
    "    mean_span=('span', 'mean'),\n",
    "    mean_std_dev=('std_dev_um', 'mean'),\n",
    "    mean_mode=('mode_um', 'mean'),\n",
    "    **{col: (col, 'mean') for col in volume_columns_percentage}\n",
    ").reset_index()\n",
    "\n",
    "# Append colors to summary table\n",
    "summary_table['color'] = summary_table['cast'].map(cast_color_map)\n",
    "\n",
    "summary_table_path = f\"{output_directory}/{campaign_code}-{location}-LISST200x-cast-summary.csv\"\n",
    "summary_table.to_csv(summary_table_path, index=False)\n",
    "print(f\"Summary table saved to: {summary_table_path}\")\n",
    "\n",
    "unique_casts = data['cast'].unique()\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(unique_casts)))\n",
    "\n",
    "print(\"Statistics per cast:\")\n",
    "display(summary_table)\n",
    "\n",
    "# Plot PSD per cast\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "for cast in unique_casts:\n",
    "    cd = data[data['cast'] == cast]\n",
    "    vals = cd[volume_columns_percentage].mean(axis=0).values\n",
    "    plt.plot(grain_sizes, vals, label=f'Cast {cast}', color=cast_color_map[cast])\n",
    "plt.title(f'LISST-200 {location} {campaign_code}: PSD per cast')\n",
    "plt.xlabel('Particle size (µm)')\n",
    "plt.ylabel('Relative volume (%)')\n",
    "plt.legend(title='Relative PSD per cast')\n",
    "plt.grid(axis='both', which='both', linewidth=0.3)\n",
    "plt.xscale('log')\n",
    "plt.tight_layout()\n",
    "psd_plot_path = f\"{output_directory}/{campaign_code}-{location}-LISST200x-PSD-per-cast.png\"\n",
    "plt.savefig(psd_plot_path, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot comparisons\n",
    "parameters = ['mean_diameter_um', 'D10_um', 'D50_um', 'D90_um', 'span', 'mode_um']\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, param in enumerate(parameters):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.boxplot(data=data, x='cast', y=param, palette=cast_color_map)\n",
    "    plt.title(f'Boxplot of {param}')\n",
    "    plt.xlabel('Cast')\n",
    "    plt.ylabel(param)\n",
    "    plt.grid(axis='both', which='both', linewidth=0.3)\n",
    "plt.tight_layout()\n",
    "psd_plot2_path = f\"{output_directory}/{campaign_code}-{location}-LISST200x-cast-comparison.png\"\n",
    "plt.savefig(psd_plot2_path, bbox_inches='tight')\n",
    "plt.show()\n",
    "                                                        ###"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
