{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c71cbc",
   "metadata": {},
   "source": [
    "# CTD on benthic lander - data processing notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e947d15",
   "metadata": {},
   "source": [
    "Disclaimer: this code is still in development and can still contain multiple errors.\n",
    "\n",
    "The following notebook was developed by the Suspended Material and Seabed Monitoring and Modelling group (SUMO) from the Institute of Natural Sciences, Belgium. It aims at providing a standard protocol to process data collected in-situ with a SBE19 plus V2 CTD instrument placed on a benthic lander. To use this notebook, simply import the raw data, run the cells containing code and inspect the results obtained. The graphs are then automatically saved as jpg and the processed data as csv.\n",
    "\n",
    "In this notebook, the CTD data processing has been divided into two stages: \n",
    "1. *Data import & flagging*: This notebook can then be used to carry out the conversion from raw to csv, which includes the import and pre-processing of the data (i.e. date conversion, data flagging). At the end of this stage, the user can save the full dataset with a flagging system and dates as datetime. \n",
    "2. *Statistics computation & visualization*: The third stage performs the calculation of different statistical parameters on a clean dataset (after filtrating data above a certain flag value), such as mean particle diameter, D10, D50, D90 and distribution characteristics, and displays outputs as graphs. The clean dataset as well as the graphs can be saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3604c",
   "metadata": {},
   "source": [
    "**Important!** Before starting the processing, make sure that all the necessary packages and libraries are installed on your computer and that you run the cell below to import everything that is required. Before running this cell, make sure you've installed all the necessary packages on your computer. To do that, you can run the following line in the prompt:\n",
    "- pip install tk pandas numpy matplotlib seaborn scipy yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0cab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 01 ##\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import shutil\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from tkinter.filedialog import asksaveasfilename\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks, savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import yaml \n",
    "import sys\n",
    "\n",
    "# Now you can import the module\n",
    "import scripts.data_processing as data_processing\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d821fc3",
   "metadata": {},
   "source": [
    "## Stage 1: Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fbf22",
   "metadata": {},
   "source": [
    "#### Stage 1 - Step 1 : Enter metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81d228",
   "metadata": {},
   "source": [
    "In this cell, the user will be asked to enter the campaign identification code, year at which the data were collected, the name of the sampling location and coordinates in decimal degrees (XX.XXXXX). Press enter after each input.\n",
    "**Important!** Avoid using \"/\" in the campaign identification code, as it might create issues when saving the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d1b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the deployment identification code: T004\n",
      "Enter the name of the sampling location: COD-T-E\n",
      "Enter the latitude of the data: x\n",
      "Enter the longitude of the data: x\n"
     ]
    }
   ],
   "source": [
    "                                                    ## Cell 02 ##\n",
    "deployment_code = input(\"Enter the deployment identification code: \")\n",
    "location = input(\"Enter the name of the sampling location: \")\n",
    "latitude = input(\"Enter the latitude of the data: \")\n",
    "longitude = input(\"Enter the longitude of the data: \")\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf1367",
   "metadata": {},
   "source": [
    "#### Stage 2 - Step 2 : Data import & formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46108881",
   "metadata": {},
   "source": [
    "Upon running this cell, a new window should automatically open and invite the user to select the csv file to be processed. Be aware that this window might open in the background and could be hidden under the current window. Once imported, the data will be attributed column names, campaign identification number and sampling location (as inserted by the user in the cell above). In addition to that, year, day, hour, minute and second columns will be converted to jday and datetime values. A new column containing a flag value (set by default to zero at this step) will be added at the end of the dataset. \n",
    "\n",
    "The dataset will be displayed upon running the cell, enabling the user to check that columns and values were assigned correctly and that the calculated date and time of the data correspond to the actual sampling time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5b925b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment Code: T004\n",
      "\n",
      "Oxygen sensor alignment default value: 2\n",
      "Working directory is: D:\\Tripod_COD_004_SBE19Plus\n",
      "Backup directory is: Z:\\2.1 Oceanographic\\T004\\T004_processed_CTD_data_BACKUP\n",
      "Processing directory folder locations:\n",
      "D:\\Tripod_COD_004_SBE19Plus\\output\\bottle\n",
      "D:\\Tripod_COD_004_SBE19Plus\\output\\screen_2Hz\n",
      "D:\\Tripod_COD_004_SBE19Plus\\output\\all_2Hz\n",
      "D:\\Tripod_COD_004_SBE19Plus\\output\\plots\n",
      "\n",
      "\u001b[1;31m*** Check filenames. Sea-Bird files present in the raw_files directory do not follow the filename convention <DEPLOYMENT_CODE>_CTD<NUMBER> ***\n",
      "\u001b[0m\n",
      "\tNumber of HEX files available in deployment folder: 1\n",
      "\n",
      "Extracting cast metadata from the header information for each cast for reference.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Extract metadata from HDR files\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtracting cast metadata from the header information for each cast for reference.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m df_NMEA \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_NMEA_from_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhdr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Check all fields populated\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mNumber of HDR files available in deployment folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_NMEA)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive - Royal Belgian Institute of Natural Sciences\\Documents\\GitHub\\counting-ocean-particles\\scripts\\data_processing.py:129\u001b[0m, in \u001b[0;36mget_NMEA_from_header\u001b[1;34m(directory, fileformat)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Save metadata to dataframe for file\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     df_NMEA \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_NMEA, \n\u001b[0;32m    125\u001b[0m                          pd\u001b[38;5;241m.\u001b[39mDataFrame([[Path(item)\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39mupper(),latdec,londec,upload_time,utc_time]], \n\u001b[0;32m    126\u001b[0m                                       columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCTD number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLat\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLong\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpload Time\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTC Time\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    127\u001b[0m                          ]\n\u001b[0;32m    128\u001b[0m                         )\n\u001b[1;32m--> 129\u001b[0m df_NMEA[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpload Time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf_NMEA\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUpload Time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m= \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    130\u001b[0m df_NMEA[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTC Time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_NMEA[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTC Time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m= \u001b[39m\u001b[38;5;124m'\u001b[39m,expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_NMEA\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "                                                    ## Cell 03 ##\n",
    "# Open a window to select input file\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "# Allow the user to select a folder\n",
    "base_path = filedialog.askdirectory(title=\"Select a directory\")\n",
    "\n",
    "if base_path:  # Check if a directory was selected\n",
    "\n",
    "    if deployment_code == '':\n",
    "        print(\"\\033[1;31m*** You have not provided a Deployment Code, please enter Deployment Code ***\\033[0m\")\n",
    "    else:\n",
    "        print(f\"Deployment Code: {deployment_code}\\n\")\n",
    "\n",
    "        # Oxygen sensor alignment is fixed at 2\n",
    "        oxygen_alignment = 2\n",
    "        print(f\"Oxygen sensor alignment default value: {oxygen_alignment}\")\n",
    "\n",
    "        # Set the cruise working directory\n",
    "        pyear = '20' + deployment_code[2:4]\n",
    "        pvessel = deployment_code[0:2]\n",
    "\n",
    "        filepath = os.path.normpath(os.path.join(base_path))\n",
    "        print(f\"Working directory is: {filepath}\")\n",
    "\n",
    "        backup_server_path = os.path.normpath(\n",
    "            os.path.join('Z:/2.1 Oceanographic', deployment_code, f\"{deployment_code}_processed_CTD_data_BACKUP\")\n",
    "        )\n",
    "        print(f\"Backup directory is: {backup_server_path}\")\n",
    "\n",
    "        # Define directory structure\n",
    "        directories = {'raw': 'raw_files', 'logs': 'logsheets', 'sbe35_raw': 'SBE35', 'cals': 'cal_samples', 'psa': 'psa', 'out': 'output'}\n",
    "        directories_out = {'screen_2Hz': 'screen_2Hz', 'all_2Hz': 'all_2Hz', 'plots': 'plots'}\n",
    "\n",
    "        def dir_path(path, name):\n",
    "            d = os.path.join(path, name)\n",
    "            if not os.path.exists(d):\n",
    "                os.mkdir(d)\n",
    "            return d\n",
    "\n",
    "        # Create processing directories\n",
    "        print(\"Processing directory folder locations:\")\n",
    "        for k, i in directories.items():\n",
    "            directories[k] = dir_path(filepath, i)\n",
    "        for k, i in directories_out.items():\n",
    "            directories_out[k] = dir_path(directories.get(\"out\"), i)\n",
    "            print(directories_out[k])\n",
    "\n",
    "        # Assign specific directory paths\n",
    "        raw = directories.get(\"raw\", \"\")\n",
    "        logs = directories.get(\"logs\", \"\")\n",
    "        psa = directories.get(\"psa\", \"\")\n",
    "        sbe35_raw = directories.get(\"sbe35_raw\", \"\")\n",
    "        bottle = directories_out.get(\"bottle\", \"\")\n",
    "        screen_2Hz = directories_out.get(\"screen_2Hz\", \"\")\n",
    "        all_2Hz = directories_out.get(\"all_2Hz\", \"\")\n",
    "        plots = directories_out.get(\"plots\", \"\")\n",
    "\n",
    "        # Check if files exist in the raw_files directory\n",
    "        if len(os.listdir(raw)) > 0:\n",
    "            count, countn = 0, 0\n",
    "            for filename in os.listdir(raw):\n",
    "                if deployment_code in filename.upper():\n",
    "                    count += 1\n",
    "                else:\n",
    "                    countn += 1\n",
    "\n",
    "            if countn != 0:\n",
    "                print(\n",
    "                    '\\n\\033[1;31m*** Check filenames. Sea-Bird files present in the raw_files directory do not follow the '\n",
    "                    'filename convention <DEPLOYMENT_CODE>_CTD<NUMBER> ***\\n\\033[0m'\n",
    "                )\n",
    "            else:\n",
    "                print(f'\\nFiles present in raw_files directory: {count}')\n",
    "        else:\n",
    "            print(\n",
    "                '\\n\\033[1;31m*** No Sea-Bird files present in the raw_files directory. If running the notebook for the first time '\n",
    "                'for this deployment, before proceeding copy across the raw SBE files to the \"raw_files\" folder in the working directory. ***\\n\\033[0m'\n",
    "            )\n",
    "\n",
    "        # Get HEX filenames\n",
    "        filelist = os.listdir(raw)\n",
    "        hexfilelist = [item.split('.')[0].upper() for item in filelist if item.endswith(\".hex\")]\n",
    "        print(f'\\tNumber of HEX files available in deployment folder: {len(hexfilelist)}')\n",
    "\n",
    "        # Extract metadata from HDR files\n",
    "        print(\"\\nExtracting cast metadata from the header information for each cast for reference.\")\n",
    "        df_NMEA = data_processing.get_NMEA_from_header(raw, 'hdr')\n",
    "\n",
    "        # Check all fields populated\n",
    "        print(f'\\tNumber of HDR files available in deployment folder: {len(df_NMEA)}')\n",
    "        df_missingNMEA = df_NMEA.isnull().sum()\n",
    "\n",
    "        for item in ['Lat', 'Long', 'Upload Time', 'UTC Time']:\n",
    "            if df_missingNMEA[item] != 0:\n",
    "                counts = df_missingNMEA[item]\n",
    "                print(\n",
    "                    f\"\\033[1;31mACTION *** {item} missing in {counts} HDR files *** Ensure {item} entered into logsheet from paper logs for:\\033[0m\"\n",
    "                )\n",
    "                print(df_NMEA[df_NMEA[item].isnull()]['CTD number'].tolist())\n",
    "            else:\n",
    "                print(f\"\\t{item} present in all HDR files\")\n",
    "                \n",
    "else:\n",
    "    print('\\n\\033[1;31m*** No directory selected. ***\\n\\033[0m')\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 03 ##\n",
    "# Open a window to select input file\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "file_path = filedialog.askopenfilename(title=\"Select a CSV file\", filetypes=[(\"CSV files\", \"*.csv\")])\n",
    "directory_path = os.path.dirname(file_path)\n",
    "\n",
    "output_directory = f\"{directory_path}/CTD-SBE19plusv2-{deployment_code}-{location}-processed\"\n",
    "if os.path.exists(output_directory):\n",
    "    shutil.rmtree(output_directory)\n",
    "    print(f'Existing output folder removed at: {output_directory}')\n",
    "os.makedirs(output_directory)\n",
    "print(f'Output folder created at: {output_directory}')\n",
    "\n",
    "if file_path:  \n",
    "    try:\n",
    "        data = pd.read_csv(file_path, header=None, sep=\" \")\n",
    "        print(\"File successfully loaded!\") \n",
    "        # Add column names\n",
    "        data.columns = [\"1.21\",\"1.6\",\"1.89\",\"2.23\",\"2.63\",\"3.11\",\"3.67\",\"4.33\",\"5.11\",\"6.03\",\"7.11\",\"8.39\",\"9.90\",\"11.7\",\"13.8\",\"16.3\",\"19.2\",\"22.7\",\"26.7\",\"31.6\",\"37.2\",\"43.9\",\"51.9\",\"61.2\",\"72.2\",\"85.2\",\"101\",\"119\",\"140\",\"165\",\"195\",\"230\",\"273\",\"324\",\"386\",\"459\",\"laser_transmission_sensor_mW\",\"supply_voltage_V\",\"external_input_1_V\",\"laser_reference_sensor_mW\",\"depth_in_m\",\"temperature_C\",\"year\",\"month\",\"day\",\"hour\",\"minute\",\"second\",\"external_input_2_V\",\"mean_diameter_um\",\"total_volume_concentration_ppm\",\"relative_humidity_%\",\"accelerometer_x\",\"accelerometer_y\",\"accelerometer_z\",\"raw_pressure_most_significant\",\"raw_pressure_least_significant\",\"ambient_light_counts\",\"external_analog_input_3_V\",\"computed_optical_transmission\",\"beam_attenuation_m\"]\n",
    "        # Add deployment identification number, sampling location name, latitude and longitude\n",
    "        data['deployment'] = deployment_code\n",
    "        data['location'] = location\n",
    "        data['latitude'] = latitude\n",
    "        data['longitude'] = longitude\n",
    "        for col in data.columns.difference(['deployment', 'location']):\n",
    "            if pd.api.types.is_numeric_dtype(data[col]) or data[col].dtype == object:\n",
    "                data[col] = data[col].astype(str).str.replace(r\"[^\\d.-]\", \"\", regex=True)  # Retain numbers, decimal points, and negatives\n",
    "                data[col] = pd.to_numeric(data[col], errors='coerce')  # Convert to numeric, invalid entries to NaN\n",
    "        # Calculate julian day & datetime\n",
    "        data['year'] = data['year'].astype(str).str.zfill(2)\n",
    "        data['month'] = data['month'].astype(str).str.zfill(2)\n",
    "        data['day'] = data['day'].astype(str).str.zfill(2)\n",
    "        data['hour'] = data['hour'].astype(str).str.zfill(2)\n",
    "        data['minute'] = data['minute'].astype(str).str.zfill(2)\n",
    "        data['second'] = data['second'].astype(str).str.zfill(2)\n",
    "        data['datetime'] = data['year'].astype(str) +\"-\"+ data['month'].astype(str) +\"-\"+ data['day'].astype(str)+\" \"+data['hour'].astype(str) +\":\"+ data['minute'].astype(str) +\":\"+ data['second'].astype(str)\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "        data['jday'] = data['datetime'].dt.dayofyear + \\\n",
    "               (data['datetime'].dt.hour / 24) + \\\n",
    "               (data['datetime'].dt.minute / 1440) + \\\n",
    "               (data['datetime'].dt.second / 86400)\n",
    "        data['flag'] = 0\n",
    "        # Compute sampling start and stop datetime\n",
    "        sampling_start = data['datetime'].min()\n",
    "        sampling_stop = data['datetime'].max()\n",
    "        # Display output\n",
    "        print(f'Data were collected at {location} between {sampling_start} and {sampling_stop} during deployment {deployment_code}.')\n",
    "        display(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "else:\n",
    "    print(\"No file selected\")\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa32723",
   "metadata": {},
   "source": [
    "## Complete CTD processing steps\n",
    "\n",
    "### The following cell performs the following tasks:\n",
    "- load data from archived SDB processed to 2 Hz CSV file into a dataframe,\n",
    "- if oxygen sensors are deployed, then advance oxygen voltage channel(s) by number of seconds determined from the interactive plots above (or the set defualt value),\n",
    "- at the end of the up-cast, flag the 'cast' where pressure is less than 2 dbar (~height of the rig) to indicate rig breaking surface (flag = 'S'),\n",
    "- load the down-cast start time (as seconds elapsed since data acquisition started) and flag the 'cast' to indicate the surface soak (flag = 'SS'),\n",
    "- drop channels labeled 'NotInUse','pumps','Start',\n",
    "- set temperatures outside range -5 to 40 degrees C as NaN,\n",
    "- recalculate practical salinities, potential temperature, sigma-theta and sound velocity (EOS-80 toolbox),\n",
    "- recalculate the oxygen concentration in umol/L and saturation using algorithms defined in SBE Application notes 64 and 64-3,\n",
    "- save data to CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a03de",
   "metadata": {},
   "source": [
    "#### Stage 2 - Step 3: Flagging of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008bcbc",
   "metadata": {},
   "source": [
    "In this cell, flagging operations are performed as follows:\n",
    "1. A first check is done on the **reference laser**, if values are below 0.2 mW, this indicates that the laser is likely not working properly. Quality flags will be assigned to 1 if data are above this value and to 3 if not (see flags meaning below). If more than 10% of the data present such a low value, a warning message will be displayed.  \n",
    "2. Data out of the water are flagged with the value of 4 (rows where the beam attenuation is equal to zero). The **beam attenuation** corresponds to the loss of light intensity when a laser beam passes through water, due to both absorption and scattering. Higher attenuation values typically indicate more turbid water. \n",
    "3. The **laser transmission** values are checked. This indicates how much of the laser light has passed through water without being absorbed or scattered. A value of 100% indicates that there has been no light loss through the sample (either water without particles or air) while values below indicate scattering and absorption (which is expected in seawater). Values above 100%, however, indicate a sensor malfunction or miscalibration. If most data (>50%) are above 100%, then a warning message is generated. Data above 100 are flagged with the value of 3. \n",
    "4. The **optical transmission** is checked: values should be ranging between 0 and 1. Values outside that range should be discarded (flag 4). However, values above 0.98-0.995 reflect extremely clear water conditions, meaning a low signal-to-noise ratio (flag 3, to be taken with caution. Above that range, values should be discarded), whereas values below 0.10 reflect very turbid data and should be discarded. \n",
    "5. Finally, a flag of 3 is attributed to **outliers** detected based on the total volume concentration, depth, optical beam transmission and temperature and comparing the value of each point to the calculated mean over a moving window of 25 points. Values are considered outliers when they are higher or lower than the moving average plus or minus three times the standard deviation.\n",
    "\n",
    "Once the flagging has been performed, a graph allows the user to visualize the quality of the data. The complete flagged dataset is automatically saved in the output directory selected by the user at the beginning of this notebook. In stage 3, a cell then allows to filter out all the data with a quality flag equal or higher than 4.\n",
    "\n",
    "Quality flags are defined following the quality flags standards defined by the NERC Environmental Data Service of the British Oceanographic Data Centre (https://vocab.nerc.ac.uk/collection/L20/current/):\n",
    "0: No quality control\n",
    "1: Good value\n",
    "2: Probably good value\n",
    "3: Probably bad value\n",
    "4: Bad value\n",
    "5: Changed value\n",
    "6: Value below detection\n",
    "7: Value in excess\n",
    "8: Interpolated value\n",
    "9: Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 04 ##\n",
    "# Step 1: Flagging based on reference_laser\n",
    "data['flag'] = data.apply(lambda row: max(row['flag'], 1) if row['laser_reference_sensor_mW'] > 0.02 else max(row['flag'], 3), axis=1)\n",
    "reference_below_thsld = (data['laser_reference_sensor_mW'] <= 0.02).sum() / len(data) * 100\n",
    "if reference_below_thsld > 20:  # If more than 20% of the data are below 0.02 mW\n",
    "    messagebox.showwarning(\"Laser should be checked\", \n",
    "                           f\"Warning: {reference_below_thsld:.0f}% of the data have a laser reference value below 0.02 mW. \"\n",
    "                           \"Please check laser.\")\n",
    "\n",
    "# Step 2: Beam attenuation flagging\n",
    "data['flag'] = data.apply(lambda row: max(row['flag'], 4) if row['beam_attenuation_m'] <= 0 else row['flag'], axis=1)\n",
    "\n",
    "# Step 3: Laser transmission flagging\n",
    "data['flag'] = data.apply(lambda row: 3 if row['laser_transmission_sensor_mW'] > 100 and row['flag'] < 3 else row['flag'], axis=1)\n",
    "underwater_data = data[data['beam_attenuation_m'] > 0]\n",
    "above_100_percent = (underwater_data['laser_transmission_sensor_mW'] > 100).sum() / len(underwater_data) * 100\n",
    "above_100_percent = (data['laser_transmission_sensor_mW'] > 100).sum() / len(data) * 100\n",
    "if above_100_percent > 50:  # If more than 50% of the data are above 100%\n",
    "    messagebox.showwarning(\"Recalibration Needed\", \n",
    "                           f\"Warning: {above_100_percent:.0f}% of the underwater data are above 100% transmission. \"\n",
    "                           \"Please recalibrate the sensor.\")\n",
    "    \n",
    "# Step 4: Optical transmission flagging\n",
    "data['flag'] = data.apply(lambda row: 3 if row['computed_optical_transmission'] > 0.98 and row['computed_optical_transmission'] < 0.995 and row['flag'] < 3 else row['flag'], axis=1)\n",
    "data['flag'] = data.apply(lambda row: 4 if row['computed_optical_transmission'] >= 0.995 and row['flag'] < 4 else row['flag'], axis=1)\n",
    "data['flag'] = data.apply(lambda row: 4 if row['computed_optical_transmission'] <= 0.10 and row['flag'] < 4 else row['flag'], axis=1)\n",
    "    \n",
    "# Step 5: Outlier detection using rolling mean and standard deviation\n",
    "columns_to_check = ['total_volume_concentration_ppm', 'temperature_C', 'depth_in_m', 'computed_optical_transmission']\n",
    "window_size = 25  # Define the window size for rolling calculations\n",
    "threshold = 3  # Define the threshold for outlier detection\n",
    "\n",
    "for col in columns_to_check:\n",
    "    rolling_mean = data[col].rolling(window=window_size, center=True).mean()\n",
    "    rolling_std = data[col].rolling(window=window_size, center=True).std()\n",
    "    outlier_column = f'is_outlier_{col}'  # Create a column to flag outliers for each variable\n",
    "    data[outlier_column] = abs(data[col] - rolling_mean) > (threshold * rolling_std)\n",
    "\n",
    "    # Update the 'flag' column if the data is an outlier\n",
    "    data['flag'] = data.apply(\n",
    "        lambda row: 3 if row[outlier_column] else row['flag'], axis=1\n",
    "    )\n",
    "\n",
    "# Combine all outlier flags into one column for visualization\n",
    "data['is_outlier'] = data[[f'is_outlier_{col}' for col in columns_to_check]].any(axis=1)\n",
    "\n",
    "# Count total outliers\n",
    "outlier_count = data['is_outlier'].sum()\n",
    "print(f'Number of outliers detected: {outlier_count}')\n",
    "\n",
    "# Warning message if quality flags exceed 75%\n",
    "percentage_high_flags = (data['flag'] >= 3).sum() / len(data) * 100\n",
    "if percentage_high_flags > 75:  # If more than 75% of the data have a quality flag of 3 or higher\n",
    "    messagebox.showwarning(\"Unsatisfactory data quality\", \n",
    "                           f\"Warning: {percentage_high_flags:.0f}% of the data have a quality flag above 3. \"\n",
    "                           \"Data should be used with caution.\")\n",
    "\n",
    "# Output\n",
    "print('Flagging has been successfully performed on the complete dataset:')\n",
    "display(data)\n",
    "\n",
    "# Visualizing outliers for each column\n",
    "for col in columns_to_check:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(data.index, data[col], label='', color='blue', s=10, alpha=0.5)\n",
    "    plt.scatter(\n",
    "        data[data[f'is_outlier_{col}']].index, \n",
    "        data[data[f'is_outlier_{col}']][col], \n",
    "        label='Outliers', color='red', marker='x', s=15\n",
    "    )\n",
    "    plt.grid(axis='both', which='both', linewidth=0.3)\n",
    "    plt.title(f'Outlier detection for {col}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(col.replace('_', ' ').title())\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizing flags\n",
    "flag_counts = data['flag'].value_counts().sort_index()\n",
    "colors = {1: 'green', 2: 'yellow', 3: 'orange', 4: 'red'}\n",
    "fig, ax = plt.subplots()\n",
    "flag_counts.plot(kind='bar', color=[colors.get(flag, 'blue') for flag in flag_counts.index], ax=ax)\n",
    "ax.grid(axis='both', which='both', linewidth=0.3)\n",
    "ax.set_title(\"Count per flag category\")\n",
    "ax.set_xlabel(\"Flag\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Save as csv\n",
    "data_path = f\"{output_directory}/{deployment_code}-{location}-LISST200x-full-data.csv\"\n",
    "if data_path:\n",
    "    data.to_csv(data_path, index=False)\n",
    "    print(f\"File saved to {file_path}\")\n",
    "else:\n",
    "    print(\"Save operation cancelled.\")\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6156e4",
   "metadata": {},
   "source": [
    "## Stage 3: Statistics computation & visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e8a0d",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 1: Statistics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653d32c",
   "metadata": {},
   "source": [
    "By running this cell, a series of statistics are calculated on the full dataset. This includes:\n",
    "1. **Total volume concentration**: This sums up the volume concentration of each grain size class per row.\n",
    "2. **Relative volume concentration**: This calculates the percentage that each class represents compared to the total volume concentration.\n",
    "3. **Mean diameter**: This corresponds to the mean diameter of particles in each row weighted by their volume concentration and normalized by the total volume concentration.\n",
    "4. **D10, D50, D90 values**: These represent the diameter below which 10%, 50% or 90% of the volume of the data are found. The D10 helps characterizing the finer fraction of the sample, the D50 corresponds to the median diameter and the D90 characterizes the coarser sediment fraction. These values are calculated based on the cumulative distribution.\n",
    "5. **Span**: It is a measure of the sorting of particle sizes, as a normalized measure of the distribution spread around the median particle size, showing the relative range of the middle 80% of the particle size distribution. Small values indicate good sorting (close to 1) while a larger span indicates poor sorting of the particles.\n",
    "6. **Standard deviation**: It measures the average dispersion of particle sizes from the mean, a greater standard deviation shows greater variability around the mean with possible outliers or tails. If the span and standard deviation are low, it shows a uniform distribution; if the span is moderate but the standard deviation is high, it shows a moderate spread with outliers; and if the span and standard deviation are high, it shows a wide distribution.\n",
    "7. **Mode**: The mode is the particle size that has the largest volume of the distribution (peak in the distribution).\n",
    "8. **Peaks in the distribution**: It identifies peaks in the particle size distribution.\n",
    "\n",
    "The particle size distribution combined with the cumulative volume concentration is displayed upon running of this cell as well as a table with the mean value of all the calculated parameters. The graph and updated complete clean dataframe are automatically saved in the directory selected by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82582c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 05 ##  \n",
    "# Calculate the total volume concentration \n",
    "volume_concentration_columns = [\"1.21\",\"1.6\",\"1.89\",\"2.23\",\"2.63\",\"3.11\",\"3.67\",\"4.33\",\"5.11\",\"6.03\",\"7.11\",\"8.39\",\"9.90\",\"11.7\",\"13.8\",\"16.3\",\"19.2\",\"22.7\",\"26.7\",\"31.6\",\"37.2\",\"43.9\",\"51.9\",\"61.2\",\"72.2\",\"85.2\",\"101\",\"119\",\"140\",\"165\",\"195\",\"230\",\"273\",\"324\",\"386\",\"459\"]\n",
    "average_volume_fractions = data[volume_concentration_columns].mean()\n",
    "volume_concentrations = data[volume_concentration_columns].values\n",
    "grain_sizes = np.array([float(col) for col in volume_concentration_columns])\n",
    "\n",
    "# Calculate the percentage of each class compared to the total volume concentration\n",
    "for col in volume_concentration_columns:\n",
    "    data[f'{col}_%'] = ((data[col] / data['total_volume_concentration_ppm']) * 100).astype(float)\n",
    "\n",
    "# Cumulative volume concentration\n",
    "cumulative_volumes = np.cumsum(data[volume_concentration_columns].values, axis=1)\n",
    "\n",
    "# Percentiles calculation function\n",
    "def calculate_d_percentile(cumulative_volumes, grain_sizes, total_volume, percentile):\n",
    "    target = total_volume * (percentile / 100.0)\n",
    "    d_percentile = []\n",
    "    for i in range(len(total_volume)):\n",
    "        if total_volume.iloc[i] == 0: \n",
    "            d_percentile.append(np.nan)\n",
    "        else:\n",
    "            greater_equal_idx = np.argmax(cumulative_volumes[i, :] >= target.iloc[i])\n",
    "            d_percentile.append(grain_sizes[greater_equal_idx])\n",
    "    return np.array(d_percentile)\n",
    "\n",
    "# Calculate D10, D50, D90\n",
    "data['D10_um'] = calculate_d_percentile(cumulative_volumes, grain_sizes, data['total_volume_concentration_ppm'], 10)\n",
    "data['D50_um'] = calculate_d_percentile(cumulative_volumes, grain_sizes, data['total_volume_concentration_ppm'], 50)\n",
    "data['D90_um'] = calculate_d_percentile(cumulative_volumes, grain_sizes, data['total_volume_concentration_ppm'], 90)\n",
    "\n",
    "# Calculate the span\n",
    "data['span'] = (data['D90_um'] - data['D10_um']) / data['D50_um']\n",
    "\n",
    "# Calculate the standard deviation\n",
    "def calculate_std(grain_sizes, volume_concentrations, mean_diameter):\n",
    "    variance = np.sum(volume_concentrations * (grain_sizes - mean_diameter[:, None])**2, axis=1) / np.sum(volume_concentrations, axis=1)\n",
    "    return np.sqrt(variance)\n",
    "data['std_dev_um'] = calculate_std(grain_sizes, volume_concentrations, data['mean_diameter_um'].values)\n",
    "\n",
    "# Calculate the mode\n",
    "def calculate_mode(grain_sizes, volume_concentrations):\n",
    "    mode_values = []\n",
    "    for vc in volume_concentrations:\n",
    "        if np.all(vc == 0):\n",
    "            mode_values.append(np.nan)\n",
    "        else:\n",
    "            mode_index = np.argmax(vc)\n",
    "            mode_values.append(grain_sizes[mode_index])\n",
    "    return np.array(mode_values)\n",
    "data['mode_um'] = calculate_mode(grain_sizes, volume_concentrations)\n",
    "\n",
    "# Identify peaks in the distribution \n",
    "def find_all_peaks(grain_sizes, volume_concentrations):\n",
    "    all_peaks = []\n",
    "    for vc in volume_concentrations:\n",
    "        if np.all(vc == 0):\n",
    "            all_peaks.append([])\n",
    "        else:\n",
    "            peaks, _ = find_peaks(vc)\n",
    "            peak_sizes = grain_sizes[peaks]\n",
    "            all_peaks.append(peak_sizes.tolist())\n",
    "    return all_peaks\n",
    "data['peaks'] = find_all_peaks(grain_sizes, volume_concentrations)\n",
    "\n",
    "# Display updated dataframe\n",
    "print('Statistics have been successfully computed')\n",
    "display(data)\n",
    "\n",
    "# Display the histogram of the particle size distribution and the cumulative volume distribution (in red)\n",
    "total_volume_concentration_per_class = data[volume_concentration_columns].sum(axis=0)\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6), dpi=300)\n",
    "ax1.plot(grain_sizes, total_volume_concentration_per_class, color='blue', marker='o', \n",
    "         label='Particle size distribution', linestyle='-')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Grain size (µm)')\n",
    "ax1.set_ylabel('Total volume concentration (µl/l)')\n",
    "ax1.set_title(f'LISST-200x {location} {deployment_code}')\n",
    "ax1.grid(axis='both', which='both', linewidth=0.3)\n",
    "cumulative_volumes = np.cumsum(total_volume_concentration_per_class)\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(grain_sizes, cumulative_volumes / cumulative_volumes[-1] * 100, color='red', \n",
    "         label='Cumulative Distribution', marker='o')\n",
    "ax2.set_ylabel('Cumulative volume (%)', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax1.axvline(data['D10_um'].mean(), color='green', linestyle='--', label='D10')\n",
    "ax1.axvline(data['D50_um'].mean(), color='orange', linestyle='--', label='D50')\n",
    "ax1.axvline(data['D90_um'].mean(), color='purple', linestyle='--', label='D90')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "graph_file_path = f\"{output_directory}/{deployment_code}-{location}-LISST200x-full-PSD.png\"\n",
    "fig.savefig(graph_file_path) \n",
    "\n",
    "# Display table with the mean of each parameter\n",
    "mean_values = {\n",
    "    'Total volume concentration (ppm)': data['total_volume_concentration_ppm'].mean(),\n",
    "    'Mean diameter (µm)': data['mean_diameter_um'].mean(),\n",
    "    'D10 (µm)': data['D10_um'].mean(),\n",
    "    'D50 (µm)': data['D50_um'].mean(),\n",
    "    'D90 (µm)': data['D90_um'].mean(),\n",
    "    'Span': data['span'].mean(),\n",
    "    'Standard deviation (µm)': data['std_dev_um'].mean(),\n",
    "    'Mode (µm)': data['mode_um'].mean()\n",
    "}\n",
    "mean_values_df = pd.DataFrame(mean_values, index=[0])\n",
    "print('Mean values of calculated parameters for the complete clean dataset:')\n",
    "display(mean_values_df)\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 12), dpi=300, sharex=True)\n",
    "axs[0].set_title(f'LISST-200x {location} {deployment_code}')\n",
    "axs[0].plot(data['datetime'], data['depth_in_m'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[0].set_ylabel('Depth (m)')\n",
    "axs[0].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[1].plot(data['datetime'], data['computed_optical_transmission'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[1].set_ylabel('Optical transmission (%)')\n",
    "axs[1].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[2].plot(data['datetime'], data['temperature_C'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[2].set_ylabel('Temperature (°C)')\n",
    "axs[2].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[3].plot(data['datetime'], data['total_volume_concentration_ppm'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[3].set_ylabel('Total volume concentration (ppm)')\n",
    "axs[3].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[4].plot(data['datetime'], data['mean_diameter_um'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[4].set_ylabel('Mean diameter (µm)')\n",
    "axs[4].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[4].set_xlabel('')\n",
    "#axs[4].xaxis.set_major_formatter(mdates.DateFormatter('%D'))\n",
    "axs[4].xaxis.set_major_locator(mdates.DayLocator(interval=7))\n",
    "fig.align_ylabels()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "graph_file_path = f\"{output_directory}/{deployment_code}-{location}-LISST200x-g-full-graph2.png\"\n",
    "fig.savefig(graph_file_path) \n",
    "\n",
    "# Save output\n",
    "print(f'Graphs are saved to: {output_directory}')\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76a71b",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 2: Manual adjustment of acceptable optical transmission, total volume concentration and/or mean diameter values (optional, upon verification on the graph above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d87d5",
   "metadata": {},
   "source": [
    "The following cell is optional and does not have to be ran if no additional filtering of the data is needed. This decision should be done following a visual inspection of the graph generated in cell 5 above. The user can define minimum and/or maximum thresholds for five possible fields (optical transmission, total volume concentration, mean diameter, pressure and/or temperature) below and/or above which the data are flagged to 4. The full dataset is saved again.\n",
    "If one threshold wants to be ignored, it can be set to -99.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0eb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 06 ##\n",
    "# Initialize thresholds\n",
    "minimum_threshold = -99.99\n",
    "maximum_threshold = -99.99\n",
    "\n",
    "# Dropdown for field selection\n",
    "field_threshold_dropdown = widgets.Dropdown(\n",
    "    options=['optical transmission', 'total volume concentration', 'mean diameter', 'depth', 'temperature'],\n",
    "    value=None,\n",
    "    description='Select field on which to apply a threshold:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Widgets for threshold inputs (initially hidden)\n",
    "min_threshold_widget = widgets.FloatText(\n",
    "    value=-99.99,\n",
    "    description='Minimum threshold:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "max_threshold_widget = widgets.FloatText(\n",
    "    value=-99.99,\n",
    "    description='Maximum threshold:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Button to apply thresholds (initially hidden)\n",
    "apply_button = widgets.Button(\n",
    "    description='Apply Thresholds',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "# Container to display widgets dynamically\n",
    "threshold_widgets_container = widgets.VBox([])\n",
    "\n",
    "# Function to handle dropdown changes\n",
    "def on_field_change(change):\n",
    "    selected_field = change['new']\n",
    "    if selected_field:\n",
    "        print(f\"Selected field: {selected_field}\")\n",
    "        # Display threshold widgets and apply button\n",
    "        threshold_widgets_container.children = [min_threshold_widget, max_threshold_widget, apply_button]\n",
    "    else:\n",
    "        # Hide threshold widgets and apply button\n",
    "        threshold_widgets_container.children = []\n",
    "\n",
    "# Observe dropdown changes\n",
    "field_threshold_dropdown.observe(on_field_change, names='value')\n",
    "\n",
    "# Function to apply thresholds\n",
    "def apply_thresholds(_):\n",
    "    global minimum_threshold, maximum_threshold\n",
    "    minimum_threshold = min_threshold_widget.value\n",
    "    maximum_threshold = max_threshold_widget.value\n",
    "    print(f\"Applying thresholds: Min = {minimum_threshold}, Max = {maximum_threshold}, Field = {field_threshold_dropdown.value}\")\n",
    "    \n",
    "    # Apply flags based on selected field\n",
    "    if field_threshold_dropdown.value == 'optical transmission':\n",
    "        if minimum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['computed_optical_transmission'] < minimum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "        if maximum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['computed_optical_transmission'] > maximum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "    elif field_threshold_dropdown.value == 'total volume concentration':\n",
    "        if minimum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['total_volume_concentration_ppm'] < minimum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "        if maximum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['total_volume_concentration_ppm'] > maximum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "    elif field_threshold_dropdown.value == 'mean diameter':\n",
    "        if minimum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['mean_diameter_um'] < minimum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "        if maximum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['mean_diameter_um'] > maximum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "    elif field_threshold_dropdown.value == 'depth':\n",
    "        if minimum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['depth_in_m'] < minimum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "        if maximum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['depth_in_m'] > maximum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "    elif field_threshold_dropdown.value == 'temperature':\n",
    "        if minimum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['temperature_C'] < minimum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "        if maximum_threshold != -99.99:\n",
    "            data['flag'] = data.apply(\n",
    "                lambda row: 4 if row['temperature_C'] > maximum_threshold and row['flag'] < 4 else row['flag'], axis=1\n",
    "            )\n",
    "\n",
    "    # Save updated data\n",
    "    data_path = f\"{output_directory}/{deployment_code}-{location}-LISST200x-full-data.csv\"\n",
    "    data.to_csv(data_path, index=False)\n",
    "    print(f\"File saved to {data_path}\")\n",
    "    print(f\"Flagging has been successfully performed and the updated dataset has been saved to: {output_directory}\")\n",
    "\n",
    "# Connect button to function\n",
    "apply_button.on_click(apply_thresholds)\n",
    "\n",
    "# Display dropdown and container\n",
    "display(field_threshold_dropdown, threshold_widgets_container)\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a2e9c",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 3: Removal of suspicious data & creation of a 'clean' dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc35311",
   "metadata": {},
   "source": [
    "This step allows the user to choose a certain flag threshold at and above which data are discarded for further analysis. The default value is set to 4, meaning all data flagged 4 and above will be removed from the filtered dataframe. In this cell, values considered outliers can also be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 07 ##\n",
    "try:\n",
    "    threshold = int(input(\"Enter the threshold for flag filtration (default is 4): \") or 4)\n",
    "except ValueError:\n",
    "    print(\"Invalid input, defaulting to threshold = 4.\")\n",
    "    threshold = 4\n",
    "\n",
    "outliers = input(\"Remove outliers (yes or no): \")\n",
    "\n",
    "# Apply initial filters\n",
    "filtered_data = data[\n",
    "    (data['total_volume_concentration_ppm'] != 0) & \n",
    "    (data['mean_diameter_um'].notna()) & \n",
    "    (data['flag'] < threshold)\n",
    "]\n",
    "\n",
    "# Additional filtering based on outliers\n",
    "if outliers.lower() == 'yes':\n",
    "    filtered_data = filtered_data[filtered_data['is_outlier'] != True]\n",
    "elif outliers.lower() != 'no':\n",
    "    print(\"Invalid input for outliers; no outlier filtering applied.\")\n",
    "\n",
    "# Display the filtered data\n",
    "print(f\"After filtering, the dataset is:\")\n",
    "display(filtered_data)\n",
    "\n",
    "# Save filtered data as csv\n",
    "filtered_data_path = f\"{output_directory}/{deployment_code}-{location}-LISST200x-clean-data.csv\"\n",
    "filtered_data.to_csv(filtered_data_path, index=False)\n",
    "print(f\"Filtered data saved to {filtered_data_path}\")\n",
    "\n",
    "# Apply Savitzky-Golay filter\n",
    "filtered_data['mean_diameter_filtered_6h'] = savgol_filter(filtered_data['mean_diameter_um'], 359, 3)\n",
    "filtered_data['mean_diameter_filtered_12h'] = savgol_filter(filtered_data['mean_diameter_um'], 719, 3)\n",
    "filtered_data['mean_diameter_filtered_24h'] = savgol_filter(filtered_data['mean_diameter_um'], 1439, 3)\n",
    "filtered_data['total_volume_filtered_6h'] = savgol_filter(filtered_data['total_volume_concentration_ppm'], 359, 3)\n",
    "filtered_data['total_volume_filtered_12h'] = savgol_filter(filtered_data['total_volume_concentration_ppm'], 719, 3)\n",
    "filtered_data['total_volume_filtered_24h'] = savgol_filter(filtered_data['total_volume_concentration_ppm'], 1439, 3)\n",
    "\n",
    "# New graph with clean and filtered data\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 12), dpi=300, sharex=True)\n",
    "axs[0].set_title(f'LISST-200x {location} {deployment_code}')\n",
    "axs[0].plot(filtered_data['datetime'], filtered_data['depth_in_m'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[0].set_ylabel('Depth (m)')\n",
    "axs[0].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[1].plot(filtered_data['datetime'], filtered_data['computed_optical_transmission'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[1].set_ylabel('Optical transmission (%)')\n",
    "axs[1].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[2].plot(filtered_data['datetime'], filtered_data['temperature_C'], color='steelblue', linestyle='-', linewidth=0.4)\n",
    "axs[2].set_ylabel('Temperature (°C)')\n",
    "axs[2].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[3].plot(filtered_data['datetime'], filtered_data['total_volume_concentration_ppm'], color='steelblue', linestyle='-', linewidth=0.4, label='Original')\n",
    "axs[3].plot(filtered_data['datetime'], filtered_data['total_volume_filtered_6h'], color='lightblue', linestyle='-', linewidth=0.6, label='6h')\n",
    "axs[3].plot(filtered_data['datetime'], filtered_data['total_volume_filtered_12h'], color='lime', linestyle='-', linewidth=0.6, label='12h')\n",
    "axs[3].plot(filtered_data['datetime'], filtered_data['total_volume_filtered_24h'], color='red', linestyle='-', linewidth=0.6, label='24h')\n",
    "axs[3].set_ylabel('Total volume concentration (µl/l)')\n",
    "axs[3].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[3].legend()\n",
    "axs[4].plot(filtered_data['datetime'], filtered_data['mean_diameter_um'], color='steelblue', linestyle='-', linewidth=0.4, label='Original')\n",
    "axs[4].plot(filtered_data['datetime'], filtered_data['mean_diameter_filtered_6h'], color='lightblue', linestyle='-', linewidth=0.6, label='6h')\n",
    "axs[4].plot(filtered_data['datetime'], filtered_data['mean_diameter_filtered_12h'], color='lime', linestyle='-', linewidth=0.6, label='12h')\n",
    "axs[4].plot(filtered_data['datetime'], filtered_data['mean_diameter_filtered_24h'], color='red', linestyle='-', linewidth=0.6, label='24h')\n",
    "axs[4].set_ylabel('Mean diameter (µm)')\n",
    "axs[4].grid(axis='both', which='both', linewidth=0.3)\n",
    "axs[4].set_xlabel('')\n",
    "axs[4].xaxis.set_major_locator(mdates.DayLocator(interval=7))\n",
    "fig.align_ylabels()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "graph_file_path = f\"{output_directory}/{deployment_code}-{location}-LISST200x-g-filtered-graph2.png\"\n",
    "fig.savefig(graph_file_path)\n",
    "                                                        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa59c69",
   "metadata": {},
   "source": [
    "#### Stage 3 - Step 4: Computation of the full PSD and evolution of the PSD with tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec5c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    ## Cell 08 ##\n",
    "# Total volume concentration per class (for full PSD)\n",
    "volume_concentration_columns = [\"1.21\",\"1.6\",\"1.89\",\"2.23\",\"2.63\",\"3.11\",\"3.67\",\"4.33\",\"5.11\",\"6.03\",\"7.11\",\"8.39\",\"9.90\",\"11.7\",\"13.8\",\"16.3\",\"19.2\",\"22.7\",\"26.7\",\"31.6\",\"37.2\",\"43.9\",\"51.9\",\"61.2\",\"72.2\",\"85.2\",\"101\",\"119\",\"140\",\"165\",\"195\",\"230\",\"273\",\"324\",\"386\",\"459\"]\n",
    "total_volume_concentration_per_class = filtered_data[volume_concentration_columns].sum(axis=0)\n",
    "average_volume_fractions = filtered_data[volume_concentration_columns].mean()\n",
    "volume_concentrations = filtered_data[volume_concentration_columns].values\n",
    "grain_sizes = np.array([float(col) for col in volume_concentration_columns])\n",
    "\n",
    "# Create figure and axis for full PSD plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6), dpi=300)\n",
    "\n",
    "# Plot the full Particle Size Distribution (PSD) on a logarithmic scale\n",
    "ax1.plot(grain_sizes, total_volume_concentration_per_class, color='blue', marker='o', \n",
    "         label='Particle size distribution', linestyle='-', markersize=5)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Grain size (µm)', fontsize=12)\n",
    "ax1.set_ylabel('Total volume concentration (µl/l)', fontsize=12)\n",
    "ax1.set_title(f'LISST-200x {location} {deployment_code}', fontsize=14)\n",
    "ax1.grid(axis='both', which='both', linewidth=0.3)\n",
    "\n",
    "# Calculate cumulative volume distribution\n",
    "cumulative_volumes = np.cumsum(total_volume_concentration_per_class)\n",
    "\n",
    "# Create secondary axis for the Cumulative Volume Distribution\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(grain_sizes, cumulative_volumes / cumulative_volumes[-1] * 100, color='red', \n",
    "         label='Cumulative Distribution', marker='o', linestyle='-', markersize=5)\n",
    "ax2.set_ylabel('Cumulative volume (%)', color='red', fontsize=12)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Add lines for D10, D50, D90 values\n",
    "ax1.axvline(filtered_data['D10_um'].mean(), color='green', linestyle='--', label='D10', linewidth=1.2)\n",
    "ax1.axvline(filtered_data['D50_um'].mean(), color='orange', linestyle='--', label='D50', linewidth=1.2)\n",
    "ax1.axvline(filtered_data['D90_um'].mean(), color='purple', linestyle='--', label='D90', linewidth=1.2)\n",
    "\n",
    "# Add legends for the two y-axes\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax2.legend(loc='upper center', fontsize=10)\n",
    "\n",
    "# Tight layout for better presentation\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Save the graph as PNG\n",
    "graph_file_path = f\"{output_directory}/{deployment_code}-{location}-LISST200x-full-PSD.png\"\n",
    "fig.savefig(graph_file_path)\n",
    "print(f\"Graph saved to {graph_file_path}\")\n",
    "\n",
    "### Plot PSD for High and Low Tide Conditions\n",
    "\n",
    "def classify_tides_with_12h_window(data):\n",
    "    # Ensure data is sorted by datetime\n",
    "    data = data.sort_values(by='datetime').reset_index(drop=True)\n",
    "    \n",
    "    # Smooth the depth data with Savitzky-Golay filter\n",
    "    sampling_interval = (data['datetime'].iloc[1] - data['datetime'].iloc[0]).total_seconds() / 3600  # In hours\n",
    "    time_interval = timedelta(hours=3)\n",
    "    window_size = int(time_interval.total_seconds() / (sampling_interval * 3600))  # Approx. 3-hour window\n",
    "    if window_size % 2 == 0:  # Ensure window size is odd\n",
    "        window_size += 1\n",
    "    \n",
    "    data['smoothed_depth'] = savgol_filter(data['depth_in_m'], window_length=window_size, polyorder=3)\n",
    "    \n",
    "    # Apply a 12-hour rolling window to identify high tides (maxima) and low tides (minima)\n",
    "    rolling_window = int(timedelta(hours=12).total_seconds() / (sampling_interval * 3600))\n",
    "    if rolling_window % 2 == 0:  # Ensure rolling window size is odd\n",
    "        rolling_window += 1\n",
    "\n",
    "    data['rolling_max'] = data['smoothed_depth'].rolling(window=rolling_window, center=True).max()\n",
    "    data['rolling_min'] = data['smoothed_depth'].rolling(window=rolling_window, center=True).min()\n",
    "    \n",
    "    # Identify high and low tides\n",
    "    data['is_high_tide'] = data['smoothed_depth'] == data['rolling_max']\n",
    "    data['is_low_tide'] = data['smoothed_depth'] == data['rolling_min']\n",
    "    \n",
    "    # Extract high tide and low tide information\n",
    "    high_tides = data[data['is_high_tide']][['datetime', 'smoothed_depth']].rename(columns={'smoothed_depth': 'high_tide_depth'})\n",
    "    low_tides = data[data['is_low_tide']][['datetime', 'smoothed_depth']].rename(columns={'smoothed_depth': 'low_tide_depth'})\n",
    "    \n",
    "    # Reset index for convenience\n",
    "    high_tides = high_tides.reset_index(drop=True)\n",
    "    low_tides = low_tides.reset_index(drop=True)\n",
    "    \n",
    "    # Combine high and low tides into one DataFrame\n",
    "    tides = pd.concat([high_tides, low_tides], keys=['high', 'low']).sort_values(by='datetime')\n",
    "    \n",
    "    # Define slack tide offset and buffer\n",
    "    slack_offset = timedelta(hours=2, minutes=30)\n",
    "    buffer = timedelta(minutes=10)\n",
    "    \n",
    "    slack_tides = []\n",
    "    ebb_tides = []\n",
    "    flow_tides = []\n",
    "    \n",
    "    # Generate slack tide times and classify ebb and flow\n",
    "    for i, row in tides.iterrows():\n",
    "        if 'high_tide_depth' in row:  # High tide\n",
    "            high_tide = row['datetime']\n",
    "            slack_tide = high_tide - slack_offset\n",
    "            slack_tides.append(slack_tide)\n",
    "            ebb_tides.append((high_tide, slack_tide))\n",
    "        elif 'low_tide_depth' in row:  # Low tide\n",
    "            low_tide = row['datetime']\n",
    "            slack_tide = low_tide - slack_offset\n",
    "            slack_tides.append(slack_tide)\n",
    "            flow_tides.append((low_tide, slack_tide))\n",
    "    \n",
    "    # Classify tides for each timestamp in the dataset\n",
    "    def classify_timestamp(timestamp):\n",
    "        for high in high_tides['datetime']:\n",
    "            if high - buffer <= timestamp <= high + buffer:\n",
    "                return \"High Tide\"\n",
    "        for low in low_tides['datetime']:\n",
    "            if low - buffer <= timestamp <= low + buffer:\n",
    "                return \"Low Tide\"\n",
    "        for slack in slack_tides:\n",
    "            if slack - buffer <= timestamp <= slack + buffer:\n",
    "                return \"Slack Tide\"\n",
    "        return \"Other\"\n",
    "    \n",
    "    # Apply classification to dataset\n",
    "    data['expected_tide'] = data['datetime'].apply(classify_timestamp)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Assuming `filtered_data` contains datetime and depth_in_m columns\n",
    "filtered_data = classify_tides_with_12h_window(filtered_data)\n",
    "\n",
    "def summarize_tides_data(filtered_data, output_directory, campaign_code, location):\n",
    "    # Create a summary table grouped by 'expected_tide' column\n",
    "    summary_table = filtered_data.groupby('expected_tide').agg(\n",
    "        mean_total_volume_concentration=('total_volume_concentration_ppm', 'mean'),\n",
    "        mean_diameter=('mean_diameter_um', 'mean'),\n",
    "        mean_D10=('D10_um', 'mean'),\n",
    "        mean_D50=('D50_um', 'mean'),\n",
    "        mean_D90=('D90_um', 'mean'),\n",
    "        mean_span=('span', 'mean'),\n",
    "        mean_std_dev=('std_dev_um', 'mean'),\n",
    "        mean_mode=('mode_um', 'mean')  # Handle volume columns\n",
    "    ).reset_index()\n",
    "\n",
    "    # Save the summary table to CSV\n",
    "    summary_table_path = f\"{output_directory}/{deployment_code}-{location}-LISST200x-tide-summary.csv\"\n",
    "    summary_table.to_csv(summary_table_path, index=False)\n",
    "    \n",
    "    print(f\"Summary table saved to: {summary_table_path}\")\n",
    "    return summary_table\n",
    "\n",
    "# Generate the summary table\n",
    "summary_table = summarize_tides_data(filtered_data, output_directory, deployment_code, location)\n",
    "display(summary_table)\n",
    "\n",
    "# Generate the plot\n",
    "volume_concentration_columns = [\"1.21_%\", \"1.6_%\", \"1.89_%\", \"2.23_%\", \"2.63_%\", \"3.11_%\", \"3.67_%\", \"4.33_%\", \"5.11_%\", \"6.03_%\", \"7.11_%\", \"8.39_%\", \"9.90_%\", \n",
    "                                \"11.7_%\", \"13.8_%\", \"16.3_%\", \"19.2_%\", \"22.7_%\", \"26.7_%\", \"31.6_%\", \"37.2_%\", \"43.9_%\", \"51.9_%\", \"61.2_%\", \n",
    "                                \"72.2_%\", \"85.2_%\", \"101_%\", \"119_%\", \"140_%\", \"165_%\", \"195_%\", \"230_%\", \"273_%\", \"324_%\", \"386_%\", \"459_%\"]\n",
    "\n",
    "# Subset data based on tide types\n",
    "high_tide_data = filtered_data[filtered_data['expected_tide'] == 'High Tide']\n",
    "low_tide_data = filtered_data[filtered_data['expected_tide'] == 'Low Tide']\n",
    "slack_tide_data = filtered_data[filtered_data['expected_tide'] == 'Slack Tide']\n",
    "other_data = filtered_data[filtered_data['expected_tide'] == 'Other']\n",
    "\n",
    "# Calculate total volume concentration for each tide type\n",
    "high_tide_concentration = high_tide_data[volume_concentration_columns].mean(axis=0)\n",
    "low_tide_concentration = low_tide_data[volume_concentration_columns].mean(axis=0)\n",
    "slack_tide_concentration = slack_tide_data[volume_concentration_columns].mean(axis=0)\n",
    "other_concentration = other_data[volume_concentration_columns].mean(axis=0)\n",
    "\n",
    "colors = {\n",
    "    'High Tide': '#ef3b2c',   # Red\n",
    "    'Low Tide': '#08306b',  # Dark Blue\n",
    "    'Slack Tide': '#74c476', # Green\n",
    "    'Other': 'lightgrey', # Grey\n",
    "}\n",
    "\n",
    "# Define line styles and alpha for prominence\n",
    "line_styles = {\n",
    "    'High Tide': {'linestyle': '-', 'alpha': 1.0},\n",
    "    'Low Tide': {'linestyle': '-', 'alpha': 1.0},\n",
    "    'Slack Tide': {'linestyle': '-', 'alpha': 1.0},\n",
    "    'Other': {'linestyle': '-', 'alpha': 0.5}\n",
    "}\n",
    "\n",
    "# Create the plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6), dpi=300)\n",
    "\n",
    "# Plot the total volume concentration for each tide type\n",
    "for tide, concentration, label in zip(\n",
    "    ['High Tide', 'Low Tide', 'Slack Tide', 'Other'],\n",
    "    [high_tide_concentration, low_tide_concentration, slack_tide_concentration, other_concentration],\n",
    "    ['High Tide', 'Low Tide', 'Slack Tide', 'Other']\n",
    "):\n",
    "    ax1.plot(\n",
    "        grain_sizes, \n",
    "        concentration, \n",
    "        color=colors[label], \n",
    "        label=label, \n",
    "        marker='o', \n",
    "        markersize=1, \n",
    "        linewidth=1.0, \n",
    "        **line_styles[label]\n",
    "    )\n",
    "\n",
    "# Set log scale for x-axis and labels for axes\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Grain size (µm)', fontsize=12)\n",
    "ax1.set_ylabel('Total volume concentration (µl/l)', fontsize=12)\n",
    "ax1.set_title(f'LISST-200x {location} {deployment_code}', fontsize=14)\n",
    "ax1.grid(axis='both', which='both', linewidth=0.3, linestyle='--', color='gray')\n",
    "\n",
    "# Calculate cumulative volume distribution for each tide type\n",
    "cumulative_volumes = {\n",
    "    'High Tide': np.cumsum(high_tide_concentration),\n",
    "    'Low Tide': np.cumsum(low_tide_concentration),\n",
    "    'Slack Tide': np.cumsum(slack_tide_concentration),\n",
    "    'Other': np.cumsum(other_concentration)\n",
    "}\n",
    "\n",
    "# Create secondary axis for the Cumulative Volume Distribution\n",
    "ax2 = ax1.twinx()\n",
    "for tide, cumulative_volume in cumulative_volumes.items():\n",
    "    ax2.plot(\n",
    "        grain_sizes, \n",
    "        cumulative_volume / cumulative_volume[-1] * 100, \n",
    "        color=colors[tide], \n",
    "        linestyle=':', \n",
    "        linewidth=0.8, \n",
    "        alpha=line_styles[tide]['alpha'], \n",
    "        label=f'Cumulative {tide}'\n",
    "    )\n",
    "\n",
    "# Set y-axis label for cumulative volume\n",
    "ax2.set_ylabel('Cumulative volume (%)', color='k', fontsize=12)\n",
    "ax2.tick_params(axis='y', labelcolor='k')\n",
    "\n",
    "# Add legends for the two y-axes\n",
    "ax1.legend(loc='upper left', fontsize=10, frameon=False)\n",
    "ax2.legend(loc='upper center', fontsize=10, frameon=False)\n",
    "\n",
    "# Tight layout for better presentation\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Save the graph\n",
    "graph_file_path = f\"{output_directory}/{deployment_code}-{location}-LISST200x-PSD-tidal-conditions.png\"\n",
    "fig.savefig(graph_file_path, bbox_inches='tight')\n",
    "print(f\"Graph saved to {graph_file_path}\")\n",
    "\n",
    "import random\n",
    "\n",
    "def plot_tide_classified_data(filtered_data):\n",
    "    # Define colors for each tide type\n",
    "    tide_colors = {\n",
    "        \"High Tide\": \"red\",\n",
    "        \"Low Tide\": \"blue\",\n",
    "        \"Slack Tide\": \"green\",\n",
    "        \"Other\": \"grey\"\n",
    "    }\n",
    "    \n",
    "    # Randomly select a day from the deployment\n",
    "    random_day = random.choice(pd.to_datetime(filtered_data['datetime']).dt.date.unique())\n",
    "    zoomed_data = filtered_data[filtered_data['datetime'].dt.date == random_day]\n",
    "\n",
    "    # Plotting the zoomed data for the random day\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for tide_type, color in tide_colors.items():\n",
    "        # Filter data for each tide type\n",
    "        tide_data = zoomed_data[zoomed_data['expected_tide'] == tide_type]\n",
    "        plt.scatter(tide_data['datetime'], tide_data['mean_diameter_um'], \n",
    "                    color=color, label=tide_type, s=10)\n",
    "    \n",
    "    # Formatting the zoomed plot\n",
    "    plt.title(f'Mean diameter vs time (Zoomed in on {random_day})')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Mean diameter (µm)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_tide_classified_data(filtered_data)\n",
    "                                                        ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
